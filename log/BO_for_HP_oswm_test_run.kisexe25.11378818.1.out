FOUND eth0 INTERFACE
Worker starting at: 20240424-122319
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364d02da440>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 4, 'relu': True, 'sigmoid': False, 'sin': False, 'state_offset': 4.411104375096299, 'state_scale': 7.873237772421341, 'tanh': False, 'test': False, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': False, 'use_res_connection': False, 'width_hidden': 48, 'dropout_p': 0.21340392192999502}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 10 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364d02da440>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 4, 'relu': True, 'sigmoid': False, 'sin': False, 'state_offset': 4.411104375096299, 'state_scale': 7.873237772421341, 'tanh': False, 'test': False, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': False, 'use_res_connection': False, 'width_hidden': 48, 'dropout_p': 0.21340392192999502}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (None, 14)  and nout 14
Using a Transformer with 13.17 M parameters
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
-----------------------------------------------------------------------------------------
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364d015a9e0>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 1, 'relu': True, 'sigmoid': False, 'sin': False, 'state_offset': 3.508183357797577, 'state_scale': 12.339951120661604, 'tanh': False, 'test': False, 'use_bias': False, 'use_dropout': False, 'use_layer_norm': False, 'use_res_connection': True, 'width_hidden': 61}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 10 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364d015a9e0>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 1, 'relu': True, 'sigmoid': False, 'sin': False, 'state_offset': 3.508183357797577, 'state_scale': 12.339951120661604, 'tanh': False, 'test': False, 'use_bias': False, 'use_dropout': False, 'use_layer_norm': False, 'use_res_connection': True, 'width_hidden': 61}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (None, 14)  and nout 14
Using a Transformer with 13.17 M parameters
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 34.88s | mean loss  0.62 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.58,-,-,-,-,-,-,-, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.61,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.71,-,-,-,-, 0.54,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.53,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.53,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.64,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 1.42, 0.70,-,-,-,-, lr of zeroth group 1.2235870926211646e-06 data time  3.05 step time  1.01 forward time  0.34 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364a765c550>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 6, 'relu': True, 'sigmoid': False, 'sin': False, 'state_offset': 2.378073090130626, 'state_scale': 15.623691307256298, 'tanh': True, 'test': False, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': False, 'use_res_connection': True, 'width_hidden': 49, 'dropout_p': 0.7344714878074746}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 10 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364a765c550>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 6, 'relu': True, 'sigmoid': False, 'sin': False, 'state_offset': 2.378073090130626, 'state_scale': 15.623691307256298, 'tanh': True, 'test': False, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': False, 'use_res_connection': True, 'width_hidden': 49, 'dropout_p': 0.7344714878074746}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (None, 14)  and nout 14
Using a Transformer with 13.17 M parameters
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 77.26s | mean loss  0.55 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.60,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.47,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-, 0.54,-,-,-,-, 0.45,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.53,-,-,-, lr of zeroth group 1.2235870926211646e-06 data time  6.26 step time  1.08 forward time  0.35 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364a765d630>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'NNEnv', 'num_hidden': 1, 'relu': False, 'sigmoid': True, 'sin': True, 'state_offset': 3.8140268079924358, 'state_scale': 14.191897216608385, 'tanh': False, 'test': False, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': False, 'width_hidden': 43, 'dropout_p': 0.539533646106763}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 10 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 10, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7364a765d630>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'NNEnv', 'num_hidden': 1, 'relu': False, 'sigmoid': True, 'sin': True, 'state_offset': 3.8140268079924358, 'state_scale': 14.191897216608385, 'tanh': False, 'test': False, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': False, 'width_hidden': 43, 'dropout_p': 0.539533646106763}}, 'get_batch_method': <function get_batch at 0x7364d02a1d80>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (None, 14)  and nout 14
Using a Transformer with 13.17 M parameters
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 57.75s | mean loss  0.64 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.70,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.83,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.85,-,-, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.66,-,-,-,-,-,-,-,-,-,-,-, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, lr of zeroth group 1.2235870926211646e-06 data time  5.38 step time  0.89 forward time  0.29 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
DONE
Finished at Wed Apr 24 12:33:32 PM CEST 2024
