FOUND eth0 INTERFACE
Worker starting at: 20240507-000417
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c844505900>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'sigmoid', 'decoder_depth': 3, 'decoder_res_connection': True, 'decoder_type': 'cat', 'decoder_use_bias': True, 'decoder_width': 64, 'encoder_activation': 'gelu', 'encoder_depth': 5, 'encoder_res_connection': True, 'encoder_type': 'mlp', 'encoder_use_bias': False, 'encoder_width': 256, 'env_name': 'MomentumEnv', 'relu': False, 'sigmoid': True, 'sin': True, 'state_offset': 4.006312139129317, 'state_scale': 6.502326311583381, 'tanh': True, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': False, 'dropout_p': 0.3221156177809492, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 100 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c844505900>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'sigmoid', 'decoder_depth': 3, 'decoder_res_connection': True, 'decoder_type': 'cat', 'decoder_use_bias': True, 'decoder_width': 64, 'encoder_activation': 'gelu', 'encoder_depth': 5, 'encoder_res_connection': True, 'encoder_type': 'mlp', 'encoder_use_bias': False, 'encoder_width': 256, 'env_name': 'MomentumEnv', 'relu': False, 'sigmoid': True, 'sin': True, 'state_offset': 4.006312139129317, 'state_scale': 6.502326311583381, 'tanh': True, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': False, 'dropout_p': 0.3221156177809492, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (<class 'owsm_worker.mlp_decoder_generator_generator.<locals>.NNDecClass'>, 14)  and nout 14
Using a Transformer with 13.59 M parameters
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 394.34s | mean loss  0.17 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.71, 0.52,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.64,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-,-,-,-,-,-,-,-, 0.53,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.73,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.88,-,-,-,-,-,-,-,-, 0.56,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.52,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.58,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.67,-,-, 0.54,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.62,-,-,-,-,-,-,-, 0.50,-,-,-,-, 0.74,-,-,-,-, 0.57,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-,-,-, 0.64,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-,-,-,-,-,-,-,-,-, 0.74,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-, 0.59,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-, 0.77, 1.02,-, lr of zeroth group 1.2458333333333336e-05 data time 273.21 step time  0.73 forward time  0.22 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
-----------------------------------------------------------------------------------------
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c844507eb0>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'sigmoid', 'decoder_depth': 4, 'decoder_res_connection': False, 'decoder_type': 'cat', 'decoder_use_bias': False, 'decoder_width': 64, 'encoder_activation': 'sigmoid', 'encoder_depth': 4, 'encoder_res_connection': True, 'encoder_type': 'cat', 'encoder_use_bias': True, 'encoder_width': 512, 'env_name': 'NNEnv', 'relu': True, 'sigmoid': True, 'sin': False, 'state_offset': 3.8901736019112376, 'state_scale': 19.27818273617494, 'tanh': True, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': True, 'dropout_p': 0.6986254309803048, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 100 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c844507eb0>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'sigmoid', 'decoder_depth': 4, 'decoder_res_connection': False, 'decoder_type': 'cat', 'decoder_use_bias': False, 'decoder_width': 64, 'encoder_activation': 'sigmoid', 'encoder_depth': 4, 'encoder_res_connection': True, 'encoder_type': 'cat', 'encoder_use_bias': True, 'encoder_width': 512, 'env_name': 'NNEnv', 'relu': True, 'sigmoid': True, 'sin': False, 'state_offset': 3.8901736019112376, 'state_scale': 19.27818273617494, 'tanh': True, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': True, 'dropout_p': 0.6986254309803048, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (<class 'owsm_worker.cat_decoder_generator_generator.<locals>.NNCatDecClass'>, 14)  and nout 14
Using a Transformer with 17.20 M parameters
Invalid epoch encountered, skipping The size of tensor a (8160) must match the size of tensor b (4760) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (10752) must match the size of tensor b (6272) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (23904) must match the size of tensor b (13944) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (6816) must match the size of tensor b (3976) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (35424) must match the size of tensor b (20664) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (36384) must match the size of tensor b (21224) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (44064) must match the size of tensor b (25704) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (9984) must match the size of tensor b (5824) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (34656) must match the size of tensor b (20216) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (8448) must match the size of tensor b (4928) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (8448) must match the size of tensor b (4928) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (22944) must match the size of tensor b (13384) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (33408) must match the size of tensor b (19488) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (7776) must match the size of tensor b (4536) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (12672) must match the size of tensor b (7392) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (13920) must match the size of tensor b (8120) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (19488) must match the size of tensor b (11368) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (14880) must match the size of tensor b (8680) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2688) must match the size of tensor b (1568) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (34560) must match the size of tensor b (20160) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (7296) must match the size of tensor b (4256) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (4416) must match the size of tensor b (2576) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (6432) must match the size of tensor b (3752) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2976) must match the size of tensor b (1736) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (16512) must match the size of tensor b (9632) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (41280) must match the size of tensor b (24080) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (3264) must match the size of tensor b (1904) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (24192) must match the size of tensor b (14112) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (41280) must match the size of tensor b (24080) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (30912) must match the size of tensor b (18032) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (44352) must match the size of tensor b (25872) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (18528) must match the size of tensor b (10808) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (26784) must match the size of tensor b (15624) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (8544) must match the size of tensor b (4984) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (26016) must match the size of tensor b (15176) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (35712) must match the size of tensor b (20832) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (480) must match the size of tensor b (280) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (5568) must match the size of tensor b (3248) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (25248) must match the size of tensor b (14728) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (20256) must match the size of tensor b (11816) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (1632) must match the size of tensor b (952) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (7776) must match the size of tensor b (4536) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (28032) must match the size of tensor b (16352) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (29664) must match the size of tensor b (17304) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (17760) must match the size of tensor b (10360) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (5760) must match the size of tensor b (3360) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (31872) must match the size of tensor b (18592) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (384) must match the size of tensor b (224) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (45312) must match the size of tensor b (26432) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (288) must match the size of tensor b (168) at non-singleton dimension 0...
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c8445808b0>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'sigmoid', 'decoder_depth': 6, 'decoder_res_connection': True, 'decoder_type': 'cat', 'decoder_use_bias': True, 'decoder_width': 64, 'encoder_activation': 'gelu', 'encoder_depth': 6, 'encoder_res_connection': True, 'encoder_type': 'cat', 'encoder_use_bias': True, 'encoder_width': 512, 'env_name': 'MomentumEnv', 'relu': True, 'sigmoid': True, 'sin': True, 'state_offset': 3.125324200870681, 'state_scale': 19.89719890883327, 'tanh': True, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': False, 'dropout_p': 0.6109857813521073, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 100 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c8445808b0>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'sigmoid', 'decoder_depth': 6, 'decoder_res_connection': True, 'decoder_type': 'cat', 'decoder_use_bias': True, 'decoder_width': 64, 'encoder_activation': 'gelu', 'encoder_depth': 6, 'encoder_res_connection': True, 'encoder_type': 'cat', 'encoder_use_bias': True, 'encoder_width': 512, 'env_name': 'MomentumEnv', 'relu': True, 'sigmoid': True, 'sin': True, 'state_offset': 3.125324200870681, 'state_scale': 19.89719890883327, 'tanh': True, 'use_bias': True, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': False, 'dropout_p': 0.6109857813521073, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (<class 'owsm_worker.cat_decoder_generator_generator.<locals>.NNCatDecClass'>, 14)  and nout 14
Using a Transformer with 19.31 M parameters
Invalid epoch encountered, skipping The size of tensor a (21024) must match the size of tensor b (12264) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (3648) must match the size of tensor b (2128) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (21120) must match the size of tensor b (12320) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (41088) must match the size of tensor b (23968) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (4800) must match the size of tensor b (2800) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (10560) must match the size of tensor b (6160) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (12288) must match the size of tensor b (7168) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (29280) must match the size of tensor b (17080) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (18624) must match the size of tensor b (10864) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (4704) must match the size of tensor b (2744) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2016) must match the size of tensor b (1176) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (10464) must match the size of tensor b (6104) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (3648) must match the size of tensor b (2128) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (17280) must match the size of tensor b (10080) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (22080) must match the size of tensor b (12880) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (5952) must match the size of tensor b (3472) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (3552) must match the size of tensor b (2072) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (8064) must match the size of tensor b (4704) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2304) must match the size of tensor b (1344) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (7872) must match the size of tensor b (4592) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (15744) must match the size of tensor b (9184) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (12384) must match the size of tensor b (7224) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (45120) must match the size of tensor b (26320) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (35808) must match the size of tensor b (20888) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (12288) must match the size of tensor b (7168) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (16992) must match the size of tensor b (9912) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (1056) must match the size of tensor b (616) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2784) must match the size of tensor b (1624) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (9792) must match the size of tensor b (5712) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (192) must match the size of tensor b (112) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2784) must match the size of tensor b (1624) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (15456) must match the size of tensor b (9016) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (23424) must match the size of tensor b (13664) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (5952) must match the size of tensor b (3472) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (28512) must match the size of tensor b (16632) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (768) must match the size of tensor b (448) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2880) must match the size of tensor b (1680) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (5376) must match the size of tensor b (3136) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (32736) must match the size of tensor b (19096) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (1440) must match the size of tensor b (840) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (480) must match the size of tensor b (280) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (10176) must match the size of tensor b (5936) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (2976) must match the size of tensor b (1736) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (26592) must match the size of tensor b (15512) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (1632) must match the size of tensor b (952) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (24096) must match the size of tensor b (14056) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (1632) must match the size of tensor b (952) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (24288) must match the size of tensor b (14168) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (13920) must match the size of tensor b (8120) at non-singleton dimension 0...
Invalid epoch encountered, skipping The size of tensor a (8256) must match the size of tensor b (4816) at non-singleton dimension 0...
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c844580c10>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'gelu', 'decoder_depth': 2, 'decoder_res_connection': False, 'decoder_type': 'mlp', 'decoder_use_bias': True, 'decoder_width': 16, 'encoder_activation': 'relu', 'encoder_depth': 2, 'encoder_res_connection': True, 'encoder_type': 'mlp', 'encoder_use_bias': False, 'encoder_width': 16, 'env_name': 'NNEnv', 'relu': True, 'sigmoid': True, 'sin': True, 'state_offset': 2.2219764510236026, 'state_scale': 3.058650342191778, 'tanh': False, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': True, 'dropout_p': 0.41037242787862427, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 100 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x72c844580c10>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'decoder_activation': 'gelu', 'decoder_depth': 2, 'decoder_res_connection': False, 'decoder_type': 'mlp', 'decoder_use_bias': True, 'decoder_width': 16, 'encoder_activation': 'relu', 'encoder_depth': 2, 'encoder_res_connection': True, 'encoder_type': 'mlp', 'encoder_use_bias': False, 'encoder_width': 16, 'env_name': 'NNEnv', 'relu': True, 'sigmoid': True, 'sin': True, 'state_offset': 2.2219764510236026, 'state_scale': 3.058650342191778, 'tanh': False, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': True, 'dropout_p': 0.41037242787862427, 'num_hidden': 1, 'width_hidden': 16, 'test': False}}, 'get_batch_method': <function get_batch at 0x72c8447af7f0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (<class 'owsm_worker.mlp_decoder_generator_generator.<locals>.NNDecClass'>, 14)  and nout 14
Using a Transformer with 12.64 M parameters
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 484.24s | mean loss  0.55 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.66,-, 0.54,-,-,-,-,-,-, 0.55,-,-,-,-,-,-,-,-,-, 0.50, 0.70, 0.55,-,-,-,-,-,-, 0.57,-,-,-,-,-,-,-,-,-, 0.56,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38, 0.68,-,-,-, 0.49,-, 0.50,-,-, 0.48,-,-,-,-, 0.61, 0.63,-,-,-,-,-, 0.37,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.58,-,-, 0.61,-, 0.55,-,-,-,-,-,-,-,-,-,-,-, 0.62, 0.48,-,-,-,-,-,-,-, 0.47,-,-,-,-, 0.50, 0.53,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-, 0.57, 0.31,-,-,-, 0.59,-,-,-,-,-,-,-, 0.69, 0.61,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.60,-,-,-,-,-,-, 0.55,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.55,-,-,-,-, 0.40,-,-, 0.60,-,-,-,-,-,-,-, 0.77,-, 0.59,-, 0.49,-,-,-,-,-,-,-,-, 0.55,-,-,-,-,-,-,-,-,-, 0.61,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.47, 0.60,-,-,-,-,-,-,-,-,-,-,-, 0.45,-, 0.57,-, 0.45,-,-,-,-,-, 0.23,-, 0.48,-,-,-,-,-,-, 0.38,-,-,-, 0.49,-,-,-,-, 0.67,-, 0.46,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-, 0.50,-, 0.71,-, 0.88,-, 0.59, 0.84,-,-,-,-,-,-,-,-,-,-, 0.46,-,-, 0.59,-,-, 0.41,-,-, 0.56,-,-,-,-,-,-,-,-,-,-,-, 0.39, 0.49,-,-,-, 0.46,-,-,-,-,-,-, 0.44,-, 0.40,-,-,-,-,-,-, 0.47, 0.41,-, 0.53,-,-,-,-,-,-,-,-,-,-, 0.51,-,-,-,-, 0.44, 0.34,-,-,-,-,-,-,-, 0.54,-, 0.29,-, 0.44,-,-, 0.37,-,-, 0.35,-,-, 0.37,-,-,-, 0.64,-, 0.41,-,-, 0.24,-,-,-, 0.39, 0.63,-, 0.47, 0.57, 0.36, 0.10, 0.15,-, lr of zeroth group 1.2458333333333336e-05 data time  3.76 step time  1.06 forward time  0.35 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 483.25s | mean loss  0.52 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-,-, 0.52,-,-,-,-,-, 0.54,-, 0.44, 0.55,-,-, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.61,-,-,-,-,-,-, 0.48,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.58,-, 0.48,-,-,-,-,-,-,-, 0.40,-, 0.86,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.59,-,-,-, 0.50,-, 0.55,-,-,-, 0.50,-,-,-,-,-,-,-,-,-,-,-,-, 0.49, 0.43,-,-,-,-,-,-, 0.79,-, 0.57,-,-, 0.55,-,-,-,-,-,-, 0.64,-,-,-,-,-,-,-, 0.74,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54, 0.55,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-, 0.47,-,-,-,-,-, 0.51,-,-,-,-,-,-,-,-,-, 0.56,-,-,-, 0.85,-,-,-,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.55,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-, 0.51, 0.41, 0.50,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-, 0.50,-,-, 0.59,-,-, 0.48, 0.33,-,-,-, 0.51,-,-,-,-, 0.63,-,-,-,-,-,-, 0.45, 0.44,-, 0.43,-,-,-,-,-, 0.49,-, 0.36,-,-,-,-,-,-,-,-,-,-, 0.66,-, 0.38,-,-,-,-,-, 0.30,-,-, 0.57,-,-,-,-,-,-, 0.49,-, 0.48,-,-,-,-,-,-,-,-,-,-,-,-, 0.60,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.56, 0.49,-,-,-,-,-, 0.72,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-,-, 0.38, 1.12, 0.50, 0.56,-,-, 0.47, 0.44,-,-, 1.74,-,-,-, 0.57,-,-,-,-, 0.42,-, 0.65, 0.28, 0.66,-, 0.50,-, 0.14, 0.23,-, lr of zeroth group 1.6625000000000004e-05 data time  4.52 step time  0.80 forward time  0.28 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 480.86s | mean loss  0.51 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.59,-,-,-, 0.32,-,-,-,-,-, 0.47,-,-,-,-, 0.57,-, 0.56,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-, 0.50,-,-,-,-,-,-,-,-,-, 0.47,-,-,-,-,-,-,-, 0.55,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.65,-,-,-,-,-,-,-,-,-,-, 0.79,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.50, 0.66,-,-,-,-,-,-,-,-, 0.51,-,-,-,-,-,-, 0.40,-,-,-, 0.51, 0.66,-,-,-,-,-,-,-,-,-,-,-,-, 0.78,-,-,-, 0.38,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-, 0.47,-, 0.31,-,-,-,-,-,-,-, 0.49,-,-,-, 0.59, 0.77,-,-,-,-, 0.56,-, 0.52,-,-,-,-, 0.46,-,-, 0.33, 0.39,-,-,-,-,-,-,-,-, 0.38,-,-,-,-, 0.49, 0.50,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-, 0.65,-, 0.55,-,-, 0.43,-, 0.61, 0.45,-,-,-, 0.54,-,-,-, 0.63,-, 0.52,-,-,-, 0.61,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.42,-,-,-,-,-,-, 0.54,-,-, 0.51,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-,-, 0.58, 0.54,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.50, 0.61, 0.60,-,-,-,-,-,-,-,-,-,-,-, 0.40,-,-,-,-,-,-, 0.65,-,-,-,-,-, 0.94,-, 0.50, 0.48,-,-,-, 0.33,-,-,-, 0.27,-,-,-, 0.64,-,-, 0.25,-,-,-,-,-,-,-,-, 0.28,-,-, 0.69, 0.46,-,-, 0.43, 0.34,-,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-, 0.49,-,-,-, 0.40, 0.76,-,-, 0.55,-,-, 0.38,-,-,-,-,-, 0.35, 0.36,-,-,-,-,-,-,-,-,-,-,-, 0.41, 0.28, 0.52, 0.49,-,-,-,-, 0.40, 0.42, 0.24, 0.57,-,-, 0.18,-,-,-, 0.17, 0.48,-, lr of zeroth group 2.0791666666666666e-05 data time  4.22 step time  0.79 forward time  0.26 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 480.88s | mean loss  0.46 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.55,-,-,-,-,-,-,-,-,-,-,-, 0.57,-, 0.28,-,-,-, 0.70,-,-,-,-, 0.45,-, 0.45,-,-,-,-,-,-,-, 0.47, 0.54,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-, 0.45,-, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.61,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.42,-, 0.36,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-, 0.69,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-, 0.50,-, 0.45, 0.30,-,-, 0.86,-,-,-,-,-,-,-, 0.43,-,-,-,-, 0.52,-,-,-,-,-,-,-,-, 0.73,-,-,-,-,-, 0.37,-,-,-,-,-,-,-,-, 0.53,-,-,-,-,-,-,-,-, 0.65,-,-,-,-,-,-,-,-,-,-,-,-, 0.50,-,-,-,-,-, 0.31,-, 0.44, 0.38,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-,-,-, 0.39,-,-,-,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-,-,-, 0.48,-,-, 0.55,-, 0.38,-,-,-,-, 0.55,-,-, 0.42,-,-,-,-,-, 0.42,-,-,-,-,-,-,-,-,-, 0.57,-,-,-,-, 0.62,-,-,-,-,-,-, 0.30,-, 0.47,-,-, 0.43,-, 0.41,-,-,-,-,-, 0.33,-, 0.53,-,-, 0.97,-,-,-,-, 0.55, 0.52,-,-,-,-,-, 0.30,-, 0.18, 0.41,-,-, 0.42,-,-,-,-,-,-,-, 0.47,-, 0.29, 0.39,-,-,-, 0.41,-,-,-,-,-,-,-,-, 0.37, 0.43,-,-,-,-, 0.49,-,-, 0.60, 0.57,-,-,-,-,-, 0.29,-, 0.40, 0.36,-,-,-, 0.32,-,-,-,-, 2.09,-,-,-,-,-,-, 0.33, 0.34, 0.57,-, 1.07, 0.59, 0.42,-,-,-, 0.33,-, 0.31,-,-,-,-,-, 0.28,-, 0.42,-, 0.30,-, 0.42,-,-,-, 0.63,-, 0.30, 0.57,-,-, lr of zeroth group 2.4958333333333338e-05 data time  3.14 step time  1.10 forward time  0.38 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 483.58s | mean loss  0.45 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-,-,-,-,-, 0.55,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-, 0.52, 0.42,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.52,-,-, 0.43,-,-,-, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-,-, 0.43, 0.44,-,-,-,-,-, 0.31,-,-,-,-,-,-,-, 0.42,-, 0.38,-,-,-,-,-, 0.57,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.35, 0.41, 0.44, 0.38,-,-,-,-,-,-,-,-,-,-,-,-, 0.68,-,-,-,-,-,-, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-, 0.52,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-, 0.31,-,-,-,-,-, 0.51, 0.36,-,-,-,-, 0.47,-,-,-,-,-,-,-, 0.49, 1.05,-,-,-,-, 0.81,-,-,-,-,-,-,-,-,-,-, 0.86,-,-,-,-, 0.45,-, 0.33,-,-,-,-,-,-,-,-, 0.29,-,-,-,-, 0.49,-,-,-, 0.43,-,-,-, 0.49, 0.67, 0.46,-,-,-, 0.52,-,-,-,-,-,-,-,-,-, 0.42,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54, 0.32,-,-,-, 0.36,-,-,-,-,-,-,-, 0.35,-,-,-,-,-,-,-, 0.46, 0.56,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-, 0.55,-,-, 0.39,-, 0.33, 0.44,-,-,-,-, 0.36, 0.38,-,-, 0.50,-,-, 0.29,-,-,-, 0.42, 0.35, 0.41, 0.52,-,-,-,-,-,-, 0.42, 0.38, 0.38,-, 0.36, 0.25,-,-,-,-,-,-,-,-, 1.43,-, 0.34,-,-,-,-,-,-,-, 0.38, 0.41, 0.51, 0.35,-,-, 0.36, 1.10, 0.28, 0.18,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.68,-, 0.64, 0.51,-,-, 0.31,-,-, 0.31, 0.49, 0.35, 0.16, 0.61,-, lr of zeroth group 2.9125000000000003e-05 data time  2.83 step time  0.83 forward time  0.27 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 490.76s | mean loss  0.45 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37, 0.41,-,-, 0.33,-, 0.31,-,-,-,-,-,-,-, 0.47,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-, 0.53,-, 0.60,-,-,-,-, 0.39,-,-,-,-, 0.48,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.56,-, 0.32,-,-, 0.42,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-, 0.42,-, 0.55,-,-, 0.28, 0.27, 0.33,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-,-, 0.35,-,-,-, 0.33, 0.51,-,-,-,-,-,-,-, 0.39, 0.50,-,-, 0.28,-,-, 0.48,-,-,-,-,-,-,-,-,-,-,-, 0.47,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-, 0.47,-,-,-,-,-,-,-,-,-,-, 0.58,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.50,-, 0.57,-,-,-,-, 0.52,-, 0.41,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-, 0.27,-,-,-,-, 0.44,-, 0.34,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-, 0.56,-,-,-,-,-,-,-,-, 0.38,-,-,-,-, 0.45,-,-, 0.42,-,-,-,-,-, 0.48,-, 0.55,-,-,-,-,-,-,-,-,-,-, 0.63,-, 0.36, 0.59,-,-,-,-,-, 0.38,-,-,-,-, 0.29,-,-,-, 0.30,-,-,-, 0.38, 0.33,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-, 0.40,-,-,-,-, 0.30,-, 1.24,-, 0.31,-, 0.48, 0.28,-,-,-,-, 0.66,-,-, 0.49, 0.41,-,-,-, 0.38,-,-,-,-,-, 0.33,-, 0.67,-, 0.46,-,-, 0.37,-,-,-,-, 0.40,-,-,-,-,-, 0.35,-,-,-, 0.63,-, 0.37, 0.41,-,-,-, 0.46,-,-,-,-,-,-,-,-, 0.34,-,-,-, 0.25,-,-, 0.45,-,-, 0.38,-, 0.54, 0.42,-, 0.47,-,-, 0.33,-,-,-, 0.66,-,-,-,-, 0.07, 1.06,-, 0.26, 0.14,-, lr of zeroth group 3.329166666666667e-05 data time  4.01 step time  0.74 forward time  0.25 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 495.44s | mean loss  0.44 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-, 0.57, 0.56,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.71,-,-,-, 0.68,-, 0.34,-,-,-,-, 0.40,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.41,-, 0.38,-,-,-,-,-,-,-,-, 0.50,-,-,-, 0.25,-,-,-,-,-,-, 0.46,-,-,-,-, 0.45,-,-,-,-,-,-,-,-,-,-,-, 0.52,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-, 0.37, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.52,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-, 0.27, 0.76, 0.51,-, 0.47,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-,-, 0.31,-, 0.44,-, 0.43,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-, 0.42,-,-,-,-,-,-,-,-,-, 0.22, 0.48,-,-,-,-,-, 0.40,-,-,-,-, 0.57,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.40, 0.31,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.47,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.91,-,-, 0.42,-,-,-,-,-,-,-, 0.85,-, 0.66,-,-,-,-,-, 0.34,-,-,-,-,-,-, 0.53,-,-,-,-,-, 0.69,-,-,-,-,-, 0.36,-, 0.53, 0.26,-,-,-,-,-,-,-,-, 0.46,-, 0.48,-,-,-,-,-,-,-,-,-,-, 0.42,-,-,-,-,-, 0.44,-,-,-, 0.52, 0.48,-,-,-,-,-, 0.57, 0.44,-,-, 1.46, 0.27, 0.34,-,-,-,-,-, 0.52,-,-,-,-, 0.31, 0.36,-,-,-,-,-, 0.62, 0.29,-, 0.29, 0.29,-,-,-, 0.41, 0.43,-,-, 0.36,-,-, 0.48, 0.52, 0.20, 0.45,-,-,-, 0.36, 0.32,-, 0.70, 0.40, 0.26,-, 0.38, 0.77,-, lr of zeroth group 3.745833333333334e-05 data time  4.08 step time  1.04 forward time  0.36 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 494.42s | mean loss  0.42 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-, 0.50, 0.67,-,-,-,-,-,-,-, 0.41,-,-,-,-,-,-,-,-,-, 0.47, 0.38,-,-,-,-,-,-,-,-, 0.50, 0.55,-,-,-,-,-,-, 0.43,-,-, 0.58,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-, 0.53,-,-,-, 0.46,-,-,-,-,-,-,-, 0.49,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33, 0.40, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.67,-,-,-,-,-, 0.65, 0.46,-,-, 0.40,-,-,-, 0.47,-,-, 0.29, 0.34,-,-,-,-,-,-, 0.32,-,-, 0.47,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.65,-,-,-,-,-,-,-,-,-,-,-, 0.22, 0.40,-, 0.34,-, 0.67,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-,-, 0.41,-,-,-,-,-,-,-,-, 0.67,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38, 0.30,-, 0.44,-,-,-,-, 0.29,-,-,-,-,-,-,-,-, 0.93,-,-,-, 0.48,-,-,-,-,-,-, 0.42,-, 0.52,-,-,-,-,-,-,-,-, 0.38, 0.36,-,-,-,-,-,-,-, 0.40, 0.35, 0.30,-, 0.59,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-, 0.76,-, 0.46,-,-,-,-, 0.83, 0.37,-,-,-,-,-,-,-, 0.30,-,-, 0.42,-,-,-,-,-, 0.57,-,-,-, 0.38,-,-,-,-,-,-, 0.30,-, 0.27,-,-,-,-,-, 0.31,-,-,-, 0.48,-,-,-, 0.36,-,-, 0.34,-, 0.60, 0.34,-,-,-, 0.22, 0.38, 0.19,-,-, 0.27,-,-,-,-,-,-,-, 0.62, 0.37, 0.25,-, 0.37, 0.20,-, lr of zeroth group 4.1625e-05 data time  4.16 step time  1.07 forward time  0.38 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 489.97s | mean loss  0.39 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32, 0.35,-,-,-,-,-,-,-,-,-,-,-,-, 0.53,-,-,-,-,-,-,-,-,-,-, 0.45,-,-, 0.58,-,-,-,-,-,-,-, 0.42,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-,-,-,-,-, 0.56,-,-,-,-,-,-,-,-,-, 0.24,-,-, 0.46,-,-,-,-,-,-,-,-,-,-, 0.34, 0.31,-,-,-,-,-, 0.29,-, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-, 0.34,-,-,-,-, 0.46, 0.38,-,-, 0.26, 0.35,-,-,-,-,-,-,-, 0.55,-,-,-,-,-, 0.39, 0.29,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.41,-,-,-,-, 0.36,-,-, 0.41,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-, 0.39,-,-,-,-, 0.41, 0.21,-, 0.20,-,-,-,-,-,-,-, 0.51,-,-,-,-,-, 0.64,-,-,-,-,-,-,-, 0.40,-,-,-,-, 0.32, 0.40,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.58,-,-,-,-, 0.30, 0.30,-,-,-, 0.37, 0.47, 0.41,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-, 0.70, 0.51, 0.49,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-,-,-,-,-, 0.20,-, 0.52,-,-,-,-,-,-, 0.29, 0.34,-,-,-, 0.26, 0.26,-,-,-,-,-,-, 0.16,-, 0.37,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-, 0.42,-,-,-, 0.29,-,-,-,-,-,-,-, 0.77,-,-, 0.29,-,-,-,-, 0.21,-,-, 0.36,-, 0.37,-,-,-,-,-, 0.51, 0.42,-, 0.22, 0.41,-,-,-,-,-,-, 0.19,-,-,-, 0.30,-, 0.52, 0.31, 0.31,-,-, 0.37,-, 0.30,-,-,-,-,-, 0.27,-, 0.22,-, 0.52, 0.27,-, 0.21,-, 0.59,-, 0.33, 1.04,-,-, 0.86,-,-,-, lr of zeroth group 4.579166666666667e-05 data time  4.06 step time  1.13 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 489.57s | mean loss  0.38 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.40,-,-, 0.34,-,-,-,-,-,-,-,-, 0.41,-,-,-,-,-,-,-, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-, 0.41, 0.44,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-, 0.41, 0.38,-,-,-,-, 0.24,-,-,-, 0.37,-,-,-,-,-,-, 0.86, 0.48, 0.24,-,-,-,-,-,-, 0.33,-,-,-,-, 0.31,-, 0.40,-,-,-,-,-,-,-,-,-,-,-,-, 0.57,-,-,-,-,-,-,-, 0.43,-,-,-, 0.39,-,-,-, 0.34,-,-,-,-,-, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-, 0.26,-, 0.25,-, 0.35, 0.41,-,-, 0.54,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-, 0.38,-,-,-, 0.41,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-, 0.35,-,-,-,-,-,-, 0.35,-,-,-, 0.36,-, 0.78,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.40,-,-,-,-,-,-,-, 0.36,-,-,-, 0.50,-,-,-,-,-,-, 0.42,-,-,-,-,-,-,-,-,-,-,-,-, 0.50,-,-,-,-, 0.26,-,-,-,-,-,-, 0.41,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-, 0.69,-,-, 0.23,-,-,-, 0.76,-,-,-,-,-,-, 0.36, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30, 0.31,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-, 0.32,-, 0.31,-,-, 0.42,-,-,-,-, 0.30,-,-,-,-,-,-, 0.27,-, 0.35, 0.46,-,-,-,-,-,-,-, 0.27, 0.41, 0.22,-,-,-, 0.30,-,-,-,-, 0.28,-, 0.38,-,-,-,-, 0.21,-, 0.44,-,-,-,-, 0.28, 0.32,-,-, 0.21,-,-, 0.21,-, 0.38,-,-, 0.20,-, 0.37, 0.38,-,-,-,-, 0.33,-,-, 0.35, 0.27, 0.62,-,-,-,-, 0.09,-, 0.15, 0.11,-, lr of zeroth group 4.995833333333334e-05 data time  3.96 step time  1.28 forward time  0.46 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 503.28s | mean loss  0.39 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-, 0.37,-,-,-,-,-,-, 0.46,-,-, 0.60, 0.73,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-, 0.47,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.42,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-, 0.30, 0.29,-,-, 0.48,-,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.35, 0.54,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-, 0.65,-,-,-,-,-,-, 0.38,-,-,-,-,-,-,-,-,-, 0.34,-, 0.27, 0.34,-, 0.44, 0.39,-, 0.69,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-, 0.31,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-, 0.30, 0.30, 0.36,-,-,-,-,-,-,-,-,-,-, 1.20,-,-,-, 0.40,-, 0.19, 0.25,-,-, 0.25,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.40, 0.42,-,-,-,-,-,-, 0.31, 0.18, 0.29, 0.30,-,-,-,-,-, 0.33,-,-,-,-,-,-, 0.31,-, 0.41,-,-, 0.25,-,-,-,-,-, 0.36,-,-, 0.40,-, 0.42,-,-,-,-,-, 0.35,-, 0.33,-,-,-,-, 0.66,-,-,-,-, 0.18,-,-, 0.30,-, 0.63,-,-,-,-,-, 0.41,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-, 0.28,-, 0.33,-,-,-,-,-,-,-, 0.30, 0.39, 0.73, 0.29, 0.51,-,-,-, 0.40,-,-,-,-,-,-,-,-, 0.35, 0.27,-,-, 0.66, 0.69,-, 0.38,-, 0.26,-,-, 0.37, 0.50,-, 0.17,-, 0.30,-,-,-, 0.20, 0.55, 0.41,-, 0.35,-, 0.36,-, 0.26,-,-,-, lr of zeroth group 4.99163105912123e-05 data time  4.41 step time  1.21 forward time  0.41 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  12 | time: 493.27s | mean loss  0.35 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-, 0.45,-,-,-,-,-,-,-, 0.37,-,-,-, 0.32, 0.23,-,-, 0.36,-,-,-,-, 0.38,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-, 0.52,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-, 0.32,-,-,-,-, 0.56,-, 0.38,-, 0.36,-, 0.26,-,-,-,-, 0.61,-,-,-,-, 0.27,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.40,-,-,-,-,-, 0.26,-,-, 0.36,-,-,-,-, 0.42,-,-,-,-,-,-,-, 0.39,-,-,-, 0.34, 0.40,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-, 0.31,-,-,-, 0.32,-,-,-,-,-,-, 0.38,-,-,-,-, 0.52,-,-, 0.27, 0.26,-,-, 0.38,-, 0.36,-,-,-,-,-,-, 0.36,-,-, 0.33,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-, 0.29, 0.30,-,-,-,-,-,-, 0.28, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.42,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-,-,-,-,-, 0.62,-, 0.40,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-, 0.37,-,-,-,-,-,-,-,-, 0.36,-,-, 0.39, 0.35,-,-, 0.35,-,-,-, 0.36,-,-,-, 0.34,-, 0.34, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-, 0.52,-,-,-, 0.27,-,-,-,-,-,-, 0.55,-, 0.33,-,-,-,-,-,-, 0.30,-,-, 0.31,-,-,-,-,-, 0.16,-, 0.56,-, 0.17, 0.29,-, 0.18, 0.23, 0.29,-, 0.59,-, 0.19, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-, 0.29,-,-, 0.21, 0.36, 0.25, 0.15, 0.40,-,-, 0.21, 0.34, 0.23,-,-,-, 0.26,-, 0.44, 0.30,-,-, 0.33, 0.62, 0.72,-,-,-, lr of zeroth group 4.9662426059912184e-05 data time  3.80 step time  1.05 forward time  0.37 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  13 | time: 492.40s | mean loss  0.34 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-, 0.30, 0.31,-,-,-, 0.37,-, 0.36,-,-,-, 0.31,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.35, 0.31, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-, 0.38,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-,-,-,-,-,-,-,-,-,-,-, 0.31, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.35,-, 0.24,-, 0.58,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-, 0.32,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-, 0.68,-,-,-, 0.23,-,-,-, 0.65,-,-,-, 0.20,-,-,-,-,-,-, 0.33,-,-, 0.58, 0.35,-, 0.39,-,-,-,-,-,-,-, 0.29,-, 0.30,-,-, 0.24,-,-,-,-, 0.21,-,-, 0.21,-,-,-,-,-,-,-, 0.27, 0.41, 0.30, 0.37,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-, 0.19,-, 0.33,-, 0.30,-,-,-,-,-,-,-, 0.62,-,-,-,-,-,-,-,-, 0.52,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-, 0.18,-, 0.26,-,-, 0.32,-,-,-, 0.41,-,-,-, 0.49, 0.96,-,-,-,-,-,-,-, 0.29,-,-,-,-,-, 0.28,-,-, 0.23, 0.25,-,-,-,-,-, 0.40,-,-, 0.26,-,-,-,-, 0.39, 0.36, 0.45, 0.29,-,-, 0.35,-,-,-,-,-, 0.42,-,-, 0.23, 0.25, 0.28, 0.33, 0.28,-,-,-,-,-, 0.23,-, 0.34, 0.22, 0.24,-, 0.12, 0.36,-,-, 0.14, 0.41, 0.47,-,-, 0.33,-, lr of zeroth group 4.9240072151251834e-05 data time  3.12 step time  1.22 forward time  0.41 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  14 | time: 489.07s | mean loss  0.33 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-, 0.29,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-, 0.41,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-, 0.47,-, 0.15, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-, 0.45,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-,-,-, 0.51,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-, 0.28,-,-, 0.34,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31, 0.40,-,-, 0.28,-,-, 0.31,-,-,-,-, 0.29,-,-,-,-, 0.34, 0.63,-,-, 0.40,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-, 0.99,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-, 0.71, 0.26, 0.27,-,-,-, 0.30,-,-, 0.29,-, 0.29,-,-,-,-,-,-,-, 0.35,-, 0.25, 0.47,-,-,-,-,-,-,-,-, 0.38,-, 0.43,-,-,-,-, 0.30,-,-,-,-, 0.26, 0.30, 0.27,-,-,-,-,-, 0.20,-,-, 0.37, 0.24,-,-,-,-,-, 0.15, 0.17, 0.23,-,-, 0.39, 0.74,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-, 0.29,-,-, 0.18,-,-, 0.37,-,-,-,-,-,-,-, 0.26,-, 0.40,-,-,-,-,-, 0.30,-, 0.24,-,-,-,-,-,-, 0.26, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-, 0.21,-, 0.29,-,-,-, 0.24,-,-,-,-,-,-, 0.16,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28, 0.23,-, 0.27,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-, 0.22,-,-,-, 0.38, 0.45,-, 0.21,-, 0.29,-, 0.42, 0.08, 0.33,-,-, 0.27,-,-, 0.18,-, 0.90, 0.38,-,-,-, lr of zeroth group 4.8652133970688636e-05 data time  3.19 step time  1.18 forward time  0.39 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  15 | time: 482.33s | mean loss  0.30 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.41,-,-,-, 0.27, 0.24,-, 0.32,-,-, 0.22,-,-, 0.28,-,-,-,-,-, 0.28,-, 0.20,-, 0.38,-, 0.10,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-, 0.23,-,-, 0.38,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-, 0.27,-,-,-,-,-,-,-, 0.27,-, 0.45,-,-,-,-,-,-,-,-, 0.20, 0.25,-,-,-, 0.29,-,-,-, 0.32,-, 0.48, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-, 0.40, 0.38,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-, 0.47,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-, 0.58, 0.25,-, 0.29,-,-,-, 0.28,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24, 0.60,-, 0.46, 0.31, 0.28,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-, 0.19,-,-,-, 0.24,-, 0.23, 0.63,-, 0.41,-,-,-,-,-,-,-,-,-,-, 0.29, 0.33,-,-,-,-,-,-,-,-,-, 0.41,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-, 0.35, 0.27,-,-,-,-, 0.41,-,-,-,-, 0.27,-,-,-,-,-, 0.24, 0.33, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-, 0.18,-,-,-, 0.28,-, 0.36,-,-, 0.12,-,-,-, 0.19,-,-,-,-,-, 0.13, 0.27,-,-,-,-,-,-,-,-, 0.72,-,-,-,-,-,-, 0.26, 0.25,-, 0.25,-, 0.19, 0.26,-,-,-,-,-,-,-,-,-,-,-, 0.20,-, 0.22,-, 0.24, 0.19, 0.10,-,-,-,-,-, 0.27,-,-, 0.17,-,-,-,-,-,-,-,-,-, 0.17,-, 0.40,-, 0.50,-, 0.09, 0.38,-, lr of zeroth group 4.7902627732157294e-05 data time  3.30 step time  1.17 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  16 | time: 490.47s | mean loss  0.30 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-, 0.28,-,-,-,-,-, 0.38,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-, 0.37,-, 0.19,-,-,-, 0.28,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-, 0.33, 0.29, 0.34,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-, 0.52,-,-,-,-,-,-, 0.29,-,-,-,-,-,-, 0.26,-,-,-, 0.24,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-, 0.25,-, 0.25,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-, 0.51,-,-,-,-, 0.27,-, 0.41,-, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-, 0.20,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-, 0.27, 0.22, 0.27,-, 0.34, 0.28,-,-,-, 0.26,-, 0.25, 0.27,-,-,-,-,-, 0.31, 0.33,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-, 0.23,-,-,-, 0.26,-,-, 0.35, 0.19,-,-,-,-,-,-,-,-,-,-,-, 0.84,-,-,-,-,-,-,-, 0.17,-,-, 0.23,-,-,-,-, 0.31,-, 0.25,-,-,-,-, 0.41,-,-,-,-,-, 0.22,-, 1.54,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-,-, 0.16, 0.26,-,-, 0.37,-,-, 0.17, 0.30,-, 0.21,-,-, 0.24, 0.28,-,-,-,-,-,-, 0.28,-,-, 0.26,-,-, 0.34,-,-,-,-, 0.20,-,-, 0.20,-,-,-,-,-, 0.33,-,-, 0.30,-, 0.19, 0.25,-,-,-, 0.16,-, 0.20,-,-, 0.22,-, 1.55,-,-,-, 0.23, 0.28, 0.47,-,-, 0.43, 0.54, 0.50,-, lr of zeroth group 4.699667332325631e-05 data time  4.83 step time  1.10 forward time  0.38 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  17 | time: 498.70s | mean loss  0.30 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-, 0.29, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-, 0.20,-,-,-, 0.23,-,-, 0.26,-,-,-, 0.24,-,-, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-, 0.23,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-, 0.22,-,-,-,-, 0.30,-,-, 0.32,-,-,-, 0.29,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-, 0.39, 0.30,-,-,-, 0.23,-,-,-, 0.55, 0.33,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-, 0.63,-,-, 0.20,-,-,-, 0.34,-,-,-,-, 0.15,-, 0.36,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-, 0.49,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.53, 0.32,-,-,-,-,-,-,-,-,-,-, 0.36,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-, 0.25,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-, 0.27,-,-,-,-, 0.33,-,-, 0.34,-, 0.21,-, 0.32,-,-,-,-,-,-,-,-,-,-, 0.45,-,-,-, 0.25,-,-,-,-,-,-,-,-,-, 0.35,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-, 0.31,-,-,-, 0.27,-,-,-,-, 0.32, 0.17,-, 0.42, 0.33,-, 0.22, 0.35,-, 0.77,-,-, 0.25,-, 0.77,-,-,-, 0.21, 0.30,-, 0.27,-,-,-, 0.29,-,-,-,-,-, 0.36,-, 0.22,-, 0.23,-,-,-,-,-,-,-,-, 0.15, 0.28,-, 0.24, 0.12,-, 0.22,-, 0.22, 0.19, 0.34,-,-,-,-,-,-, 0.20, 0.19,-, 0.15,-, 0.46,-,-,-, 0.14,-, 0.18,-, 0.19,-,-,-,-,-,-, 0.12, 0.39,-,-,-, lr of zeroth group 4.594045933122417e-05 data time  3.86 step time  1.06 forward time  0.35 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  18 | time: 500.61s | mean loss  0.30 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-, 0.48,-,-,-,-,-, 0.21,-,-,-, 0.31,-,-,-,-,-,-,-,-, 0.34,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-, 0.42, 0.34,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-, 0.36,-,-,-,-,-,-, 0.41, 0.26, 0.31, 0.19,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-, 0.26,-, 0.35, 0.25,-,-,-,-, 0.28,-,-, 0.19,-,-,-,-,-,-, 0.64,-,-,-,-, 0.43,-,-, 0.30,-, 0.36, 0.14,-, 0.19,-, 0.27,-,-,-,-,-,-, 0.31,-,-, 0.19,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-, 0.21,-,-,-,-, 0.28,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30, 0.29,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-, 0.29, 0.26,-,-,-,-,-,-, 0.28, 0.24,-, 0.27,-,-,-,-,-, 0.17,-,-,-,-, 0.21,-,-,-, 0.23,-,-, 0.27,-,-,-,-,-, 0.17,-,-,-,-,-, 0.50,-,-, 0.39,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-, 0.62,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-, 0.29,-, 0.23,-,-,-,-,-,-, 0.40,-,-,-, 0.36, 0.27,-,-,-,-,-,-,-,-,-,-, 0.21, 0.25,-,-,-,-, 0.22,-, 0.15,-,-,-,-,-,-,-,-, 0.31,-,-,-,-,-, 0.23,-,-, 0.34,-,-,-, 0.13, 0.16,-,-,-,-,-,-,-,-, 0.28,-, 0.28,-,-,-, 0.37,-, 0.18, 0.39,-,-,-,-, 1.50,-, 0.17,-,-,-, 0.35, 0.12,-, 0.32, 0.15, 0.34, 0.58, 0.25, 0.17, 0.19, 0.03, 0.39,-, lr of zeroth group 4.474120076861335e-05 data time  3.33 step time  1.05 forward time  0.34 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  19 | time: 501.29s | mean loss  0.30 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-, 0.39,-,-,-,-,-,-,-, 0.61,-, 0.24,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-, 0.41,-,-, 0.16,-,-,-,-, 0.34,-, 0.10,-,-,-, 0.24,-,-,-,-,-, 0.26,-,-, 0.17,-,-,-,-,-, 0.35, 0.21,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-, 0.36,-,-,-, 0.15,-, 0.22,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26, 0.28,-,-,-,-,-,-,-,-,-, 0.50,-,-, 0.34,-,-,-,-, 0.37,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-, 0.29,-, 0.19,-,-,-,-, 0.28,-,-,-,-,-,-,-, 0.27,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.72,-,-,-,-,-,-, 0.18,-,-, 0.30, 0.14,-, 0.49,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-, 0.28, 0.41,-,-,-,-, 0.23, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-, 0.32,-,-,-, 0.32,-,-,-, 0.37,-,-,-,-, 0.27,-, 0.22, 0.37, 0.23, 0.19,-, 0.36,-,-,-,-,-, 0.16, 0.28,-,-,-,-,-,-, 0.27, 0.14,-,-, 0.67,-,-,-, 0.30,-, 0.18,-, 0.15, 0.23,-,-,-,-, 0.22, 0.52,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-, 0.16,-,-,-,-, 0.20,-, 0.16,-,-,-,-, 0.36, 0.17,-,-, 0.28, 0.31,-, 0.53,-,-, 0.21, 0.36,-,-, 0.38, 0.20, 0.47,-,-, 0.26, 0.15,-,-,-, 0.56, 0.27, 0.56, 0.28, 0.37,-, 0.36,-, lr of zeroth group 4.3407089787438646e-05 data time  4.08 step time  0.78 forward time  0.27 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  20 | time: 501.83s | mean loss  0.27 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-, 0.25,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22, 0.41,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17, 0.19,-,-,-,-,-,-,-, 0.25,-,-,-, 0.28, 0.34, 0.22,-,-,-,-,-,-,-,-,-,-,-,-, 0.42,-, 0.21,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-, 0.44,-, 0.23,-,-,-,-,-,-, 0.17, 0.15,-, 0.27, 0.50, 0.26,-,-,-,-,-,-, 0.26, 0.21,-,-, 0.44, 0.37,-,-,-,-,-, 0.12, 0.24,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-, 0.38,-,-,-,-,-,-,-,-,-,-, 0.34, 0.33,-,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-, 0.23,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-, 0.17, 0.24,-,-,-,-,-,-, 0.27,-,-, 0.27,-,-,-,-,-,-,-, 0.24,-,-,-, 0.51,-, 0.20,-,-,-,-,-,-, 0.38,-, 0.25,-,-,-, 0.15,-,-,-,-, 0.29,-, 0.15,-,-, 0.16,-,-, 0.17,-,-,-, 0.26,-,-, 0.26, 0.22,-, 0.24,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-, 0.23,-, 0.35,-,-, 0.10,-,-, 0.11,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-, 0.21, 0.27,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-, 0.82,-,-,-, 0.24,-, 0.37,-,-,-,-,-,-,-, 0.37,-,-,-,-, 0.12,-,-, 0.35, 0.45,-, 0.19, 0.60,-,-,-,-, 1.10, 0.21,-,-,-,-,-,-,-, 0.35,-, 0.28, 0.20, 0.18, 0.43,-, 0.05,-,-, lr of zeroth group 4.1947239718472256e-05 data time  3.72 step time  0.88 forward time  0.31 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  21 | time: 499.27s | mean loss  0.26 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-, 0.22, 0.25,-,-,-,-,-,-,-,-,-, 0.22,-,-,-, 0.16,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.34, 0.39,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-, 0.17,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-, 0.28,-,-, 0.16, 0.29,-,-, 0.32,-, 0.14,-,-,-,-,-,-, 0.22,-, 0.24, 0.29,-,-,-, 0.34,-,-, 0.22, 0.30,-,-, 0.21,-,-, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-, 0.35, 0.42,-, 0.23,-,-,-, 0.16, 0.55,-, 0.34, 0.18,-,-,-,-,-,-,-,-, 0.15, 0.16,-,-,-, 0.22, 0.21,-,-, 0.42, 0.22, 0.30,-,-,-,-,-,-, 0.23, 0.37,-,-,-,-,-,-,-,-,-,-, 0.15,-, 0.16, 0.38, 0.23,-,-,-, 0.26,-,-,-,-,-,-, 0.18,-,-,-,-,-,-, 0.21,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-, 0.70,-, 0.29,-,-,-,-, 0.15,-,-, 0.22,-,-,-,-,-,-, 0.17, 0.21,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-, 0.15,-,-,-, 0.19,-,-,-, 0.25, 0.26,-, 0.14,-,-,-,-,-,-,-, 0.27,-, 0.38, 0.22,-,-,-, 0.16,-, 0.15,-,-, 0.09,-, 0.27,-,-, 0.93,-, lr of zeroth group 4.037162281795368e-05 data time  4.03 step time  1.14 forward time  0.39 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  22 | time: 484.83s | mean loss  0.25 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-, 0.20, 0.21,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-, 0.26,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-, 0.22,-, 0.36,-,-,-,-,-,-,-,-,-,-,-, 0.26, 0.13, 0.19,-, 0.32,-,-, 0.40,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-, 0.26,-, 0.38,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-, 0.24,-,-,-,-,-,-,-, 0.22,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-,-,-,-, 0.35,-,-,-, 0.14,-,-,-,-, 0.21,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-, 0.19, 0.16, 0.43,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-, 0.19,-,-,-,-,-,-,-,-,-, 0.19, 0.21,-, 0.19,-,-,-,-,-, 0.23,-,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-, 0.38, 0.33,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-, 0.23,-,-, 0.18, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-, 0.52,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25, 0.39,-,-, 0.15,-,-, 0.21,-,-,-,-,-,-,-,-, 0.21,-,-, 0.22, 0.18,-,-,-,-,-,-, 0.30,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-, 0.13,-,-,-, 0.19, 0.21,-,-, 0.20,-,-, 0.19,-,-,-,-,-,-,-,-, 0.19, 0.16,-, 0.25, 0.21,-,-,-, 0.18, 0.13,-,-, 0.21,-,-,-,-,-, 0.23,-,-,-, 1.97,-, 0.07,-, 0.19, 0.13, 0.16,-, 0.99,-, 0.33,-, 0.24, 0.21, 0.54,-, lr of zeroth group 3.869100214696801e-05 data time  4.10 step time  1.19 forward time  0.41 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  23 | time: 493.55s | mean loss  0.24 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-,-, 0.21,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-, 0.37,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-, 0.21, 0.16,-,-,-,-,-, 0.20, 0.28,-,-,-,-,-, 0.12,-,-,-,-,-,-,-,-, 0.24,-,-,-, 0.18,-, 0.25, 0.10,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-, 0.21, 0.16,-, 0.22,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-, 0.39,-, 0.16,-,-,-,-,-, 0.19,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.56,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-, 0.22,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-, 0.23,-,-,-,-,-, 0.23,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.18,-,-,-, 0.32,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-, 0.26,-,-,-, 0.19, 0.24,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-, 0.29, 0.24,-,-,-, 0.13,-,-,-, 0.44, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-, 0.21,-,-, 0.21,-,-,-, 0.22,-,-,-, 0.32,-,-,-,-,-, 0.20,-, 0.22, 0.22,-, 0.25,-,-,-, 0.21,-,-,-,-,-, 0.20,-, 0.35,-,-,-, 0.21,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23, 0.65, 0.16,-,-,-,-,-,-, 0.12, 0.26,-,-,-, 0.11, 0.19,-,-,-, 0.17,-,-,-,-, 0.27, 0.13,-,-,-, 0.29,-, 0.21,-, 0.20,-,-,-, 0.19,-,-,-,-, 0.22, 0.22, 0.40, 0.16,-,-, 3.31,-, lr of zeroth group 3.6916858048824996e-05 data time  3.60 step time  1.26 forward time  0.42 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  24 | time: 494.18s | mean loss  0.24 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.41,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-, 0.19, 0.22,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-, 0.17,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-, 0.11,-,-,-,-,-,-, 0.28,-, 0.15,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-, 0.12,-,-, 0.42,-,-, 0.26, 0.18,-,-,-,-,-,-, 0.41, 0.17, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-, 0.20,-,-, 0.27,-,-, 0.16,-,-,-, 0.18,-,-,-, 0.30,-,-, 0.16,-,-, 0.17,-, 0.22,-,-, 0.16,-,-,-,-, 0.26,-,-,-,-, 0.17,-,-,-, 0.34,-, 0.24, 0.27, 0.34, 0.35,-,-, 0.24,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-, 0.42,-,-,-, 0.17,-,-, 0.40,-,-,-,-,-, 0.20, 0.19,-,-,-,-,-,-, 0.29,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-, 0.21, 0.21,-, 0.17,-,-,-,-,-, 0.12,-, 0.38,-,-,-,-, 0.21,-, 0.11, 0.13, 0.34,-,-,-,-,-, 0.16, 0.12,-, 0.11,-, 0.16,-, 0.39,-, 0.17,-, 0.05,-,-,-,-,-, 0.22,-,-, 0.32, 0.47,-,-,-, 1.68, 0.55, 0.12,-, lr of zeroth group 3.506130972667342e-05 data time  3.36 step time  1.22 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  25 | time: 499.89s | mean loss  0.24 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-, 0.19,-, 0.29,-,-,-,-,-,-,-,-, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20, 0.32,-,-,-,-, 0.38,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-, 0.17,-,-, 0.25,-, 0.34,-,-,-,-, 0.22,-,-, 0.16,-,-,-, 0.25, 0.41,-,-, 0.27,-,-, 0.24,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-, 0.13,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-, 0.17,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20, 0.21,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-, 0.34,-,-,-, 0.20,-,-,-, 0.21,-, 0.31,-,-,-,-, 0.36,-, 0.11, 0.11,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-, 0.11,-, 0.20,-,-,-,-,-, 0.15, 0.29,-,-,-,-, 0.17,-,-,-,-, 0.24,-,-, 0.31,-,-, 0.12,-,-,-,-,-,-, 0.10,-, 0.24,-,-,-,-,-,-,-,-, 0.18, 0.33,-,-,-,-,-,-,-,-,-,-,-,-, 0.12, 0.27,-,-,-, 0.25,-,-,-, 0.17,-, 0.27,-, 0.48,-,-, 0.24,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-, 0.23,-,-,-, 0.23,-,-,-,-, 0.17, 0.28, 0.23,-,-,-,-, 0.22,-,-, 0.18, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21, 0.16,-,-,-,-,-,-,-, 0.23, 0.57,-,-, 0.27, 0.44, 0.17,-,-,-,-,-,-,-, 0.14,-, 0.20,-,-,-,-,-,-,-, 0.14,-, 0.15,-,-,-, 0.08,-, 0.13,-,-,-,-,-, 0.10, 0.68,-, 0.05, 0.29, 0.29, 0.51, 0.02, 0.17,-, lr of zeroth group 3.31370324570548e-05 data time  3.48 step time  1.06 forward time  0.37 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  26 | time: 500.44s | mean loss  0.23 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20, 0.18,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-, 0.20,-,-,-,-,-,-,-, 0.27,-,-,-,-,-, 0.28,-,-, 0.15,-,-, 0.22,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-, 0.27, 0.24,-,-,-,-,-,-,-,-,-,-, 0.19,-,-, 0.24, 0.44,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-, 0.37, 0.64,-,-,-,-, 0.17,-,-,-,-,-,-,-, 0.28,-, 0.19,-,-,-, 0.19,-,-, 0.20,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-,-,-, 0.21,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-, 0.49,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-, 0.35,-,-,-,-, 0.22,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.09,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-, 0.30, 0.30,-,-,-, 0.16,-,-, 0.19, 0.20,-, 0.19,-,-,-,-,-,-,-, 0.51, 0.26, 0.31, 0.21, 0.58,-,-,-,-,-, 0.11,-,-,-, 0.17, 0.08,-, 0.21, 0.44,-, 0.18,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-, 0.19,-,-,-,-,-,-, 0.11,-,-,-, 0.14,-,-, 0.18,-,-,-,-, 0.23, 0.13,-,-,-,-,-,-,-,-, 0.13,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.35,-, 0.21, 0.12,-, 0.30, 0.17, 0.16,-, 0.21, 0.12,-,-,-, 0.15,-, 0.37, 0.05, 0.20,-, 0.45,-, lr of zeroth group 3.115717100491215e-05 data time  4.04 step time  1.18 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  27 | time: 505.30s | mean loss  0.23 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.08,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-, 0.19, 0.19,-, 0.21,-,-,-,-,-, 0.12,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-, 0.10,-,-,-,-,-, 0.20, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-, 0.31,-,-,-,-,-, 0.29,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-, 0.20,-, 0.20, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-, 0.11, 0.20,-,-,-, 0.30,-, 0.17, 0.18,-,-,-,-,-, 0.34,-, 0.18,-,-,-,-,-,-, 0.26, 0.17,-,-,-,-, 0.19, 0.22,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-, 0.26,-, 0.16,-,-, 0.17, 0.24,-,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-, 0.28,-, 0.16,-,-,-,-,-, 0.24,-, 0.15, 0.19,-,-,-,-,-,-, 0.23,-, 0.37,-,-,-,-, 0.15, 0.17,-,-,-, 0.55,-, 0.14,-,-, 0.20,-,-,-,-,-, 0.17, 0.23,-,-, 0.18,-,-, 0.21, 0.30,-, 0.25,-,-,-,-, 0.31,-, 0.16,-,-,-,-, 0.17,-,-,-, 0.18,-,-, 0.17, 0.37,-,-, 0.15,-,-,-,-, 0.15,-,-, 0.86,-, 0.14, 0.14,-, 0.29,-,-,-,-, 0.26, 0.25, 0.15, 0.12, 0.21, 0.06, 0.37,-,-, 0.27, 0.05, 0.06, 0.42,-, lr of zeroth group 2.9135249831516675e-05 data time  3.68 step time  1.20 forward time  0.41 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  28 | time: 494.67s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-, 0.29,-, 0.13,-,-,-,-,-,-, 0.18,-, 0.18,-,-,-, 0.16,-,-,-, 0.23,-, 0.19,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-, 0.23, 0.17,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-, 0.18,-,-,-, 0.24,-,-,-,-,-,-,-, 0.15,-,-,-, 0.17,-,-,-, 0.17,-,-,-,-,-,-, 0.18,-,-,-,-,-,-, 0.14,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-, 0.31,-, 0.26,-,-,-,-,-,-,-,-, 0.25,-,-, 0.20,-, 0.31,-,-,-,-,-,-,-,-, 0.25,-,-, 0.22,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-, 0.13,-,-, 0.19,-,-, 0.13,-,-, 0.17, 0.19,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-, 0.23,-,-, 0.76,-,-,-,-,-, 0.79,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-, 0.29,-,-,-,-, 0.10,-,-, 0.14,-,-,-,-,-, 0.13,-,-,-, 0.17,-,-,-,-, 0.16, 0.20,-,-,-,-,-,-,-,-,-,-,-, 0.21, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-, 0.23,-,-,-,-, 0.29,-, 0.14,-,-,-, 1.05, 0.24, 0.18, 0.26, 0.22,-,-,-,-,-,-,-, 0.22,-,-, 0.13,-,-,-, 0.10,-,-,-,-, 0.21, 0.21,-, 0.28,-,-,-,-,-,-,-,-,-,-,-, 0.27, 0.13,-, 0.26,-,-,-,-, 0.17,-, 0.20, 0.17,-,-,-, 0.28,-,-,-,-, 0.18,-,-, 0.18,-,-, 0.05, 0.08, 0.12,-, 0.43,-, 0.22, 0.14, 0.23,-, lr of zeroth group 2.708508070868378e-05 data time  4.90 step time  1.01 forward time  0.35 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  29 | time: 490.22s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-, 0.16,-,-,-, 0.16, 0.18,-,-,-,-,-, 0.22, 0.26,-,-,-,-,-,-,-,-,-, 0.18, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-, 0.21, 0.18,-,-,-,-, 0.37,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-, 0.22,-,-,-,-, 0.14, 0.18,-,-,-, 0.39, 0.12,-,-,-, 0.22,-,-,-,-, 0.19,-,-,-,-,-,-,-,-, 0.18,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-, 0.16,-, 0.15,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-,-,-,-, 0.19,-, 0.08, 0.42, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-, 0.21,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-, 0.17,-,-, 0.16, 0.17,-, 0.26,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-, 0.23,-,-, 0.19,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-, 0.33, 0.13, 0.24, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-, 0.15,-,-,-,-, 0.16,-,-, 0.09,-,-, 0.17,-,-,-,-,-,-,-,-,-,-, 0.34,-, 0.33,-,-,-,-,-,-,-, 0.16, 0.22,-,-,-,-,-,-, 0.20,-,-, 0.38,-,-,-, 0.29,-,-, 0.14,-, 0.13,-,-,-, 0.18,-,-,-,-, 0.18,-, 0.26,-,-,-,-,-,-,-,-,-,-, 0.22, 0.34,-,-, 0.37,-,-,-,-, 0.07,-,-,-,-,-, 0.19,-,-,-,-, 0.20, 0.42,-,-, 0.13, 0.48,-, 1.27, 0.18,-,-,-, 0.23, 0.16,-, 0.11,-, 0.16,-,-, 0.15,-, 0.27, 0.16, 0.25,-, lr of zeroth group 2.5020668370366547e-05 data time  3.26 step time  0.85 forward time  0.29 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  30 | time: 490.99s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-,-,-, 0.23,-,-,-, 0.27,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34, 0.18,-,-,-,-, 0.28,-,-, 0.40,-, 0.29,-,-,-,-,-,-,-, 0.33,-,-, 0.20,-,-,-, 0.10,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18, 0.21,-,-, 0.27,-,-,-,-, 0.22,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-, 0.18, 0.15,-,-, 0.15,-,-, 0.25,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-, 0.14,-,-,-, 0.23, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-, 0.25,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-, 0.25,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-, 0.15,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-, 0.18,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-, 0.20, 0.39,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-, 0.20,-,-,-,-,-,-,-,-, 0.19, 0.31,-,-,-,-, 0.22,-,-, 0.35, 0.14,-,-,-,-,-,-, 0.21,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-, 0.23,-, 0.19,-,-,-,-,-,-,-,-,-,-,-, 0.36,-,-,-, 0.20, 0.28,-,-,-,-,-, 0.14,-, 0.20,-,-, 0.19,-, 0.11,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-, 0.49,-,-, 0.25, 0.16,-,-,-,-,-, 0.26,-,-,-, 0.14, 0.20,-,-,-, 0.16, 0.45,-,-,-, 0.31, 0.09, 0.15,-, 0.13,-,-,-,-,-,-,-, 0.18, 0.14,-,-,-, 0.13,-,-, 0.14,-,-,-, 0.23,-,-,-,-,-,-, 0.16,-,-,-,-,-, 0.06,-,-,-,-,-,-,-,-, 0.79,-, 0.09,-,-, 0.31,-, 0.32, 0.13,-,-, lr of zeroth group 2.2956114846122265e-05 data time  3.57 step time  1.21 forward time  0.39 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  31 | time: 493.58s | mean loss  0.23 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-, 0.21,-, 0.24,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.80,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-, 0.30, 0.21,-, 0.20,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-, 0.16,-,-,-, 0.15,-,-, 0.14,-, 0.19,-,-,-,-,-,-, 0.15,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-, 0.15, 0.17,-,-,-,-, 0.15,-,-,-,-,-, 0.28,-,-, 0.12,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.50,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-, 0.24,-,-,-,-,-, 0.22,-,-,-,-,-, 0.31, 0.36,-, 0.18,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-, 0.15,-,-,-, 0.14, 0.18, 0.27, 0.18, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-, 0.18,-,-, 0.19, 0.17,-,-,-,-,-,-, 0.21, 0.11,-,-, 0.12,-,-,-, 0.27,-,-,-,-,-,-, 0.16, 0.20,-, 0.19,-,-,-,-,-,-,-,-, 0.08,-,-,-, 0.18,-,-, 0.18,-,-, 0.29, 0.29, 0.18,-,-,-,-,-,-,-,-,-, 0.27,-,-, 0.17,-,-,-,-,-, 0.23,-,-,-, 0.18, 0.22,-,-,-,-, 0.48,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-, 0.10, 0.16,-,-, 0.18,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13, 0.19, 0.26, 0.22, 0.10, 0.15, 0.12,-,-,-, 0.16, 0.08,-, 0.14,-,-, 0.24,-,-, 0.03, 0.26,-, lr of zeroth group 2.0905523129951237e-05 data time  4.39 step time  0.89 forward time  0.32 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  32 | time: 496.56s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21, 0.16,-,-,-,-,-, 0.11,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-, 0.14, 0.25,-,-,-,-, 0.29,-,-,-,-,-, 0.18, 0.24, 0.17,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-, 0.22,-,-,-, 0.16,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-, 0.24,-, 0.19,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-, 0.11, 0.20,-, 0.32,-, 0.17, 0.15,-,-, 0.11,-,-,-,-, 0.25,-,-, 0.25,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-, 0.20, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-, 0.17,-,-,-, 0.40,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-, 0.17,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-, 0.17,-,-, 0.14,-, 0.11,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-, 0.19,-, 0.19, 0.36,-,-,-,-,-,-,-, 0.46, 0.27,-,-,-,-,-,-, 0.26,-, 0.14,-, 0.14, 0.29,-, 0.17,-,-,-,-, 0.19,-, 0.16, 0.18, 0.12,-,-,-, 0.14, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-, 0.17,-,-,-,-,-,-,-, 0.13, 0.19,-,-, 0.11,-,-,-,-, 0.15,-, 0.15,-,-,-, 0.11,-,-,-, 0.18,-,-,-,-,-, 0.25, 0.18,-,-, 0.22,-, 0.15,-,-,-, 0.16,-,-,-, 0.18,-, 0.29, 0.15,-,-,-,-,-, 0.16,-, 0.15,-, 0.20, 0.23, 0.16,-, 0.22,-, 1.09, 0.15,-, 0.13, 0.09,-,-, lr of zeroth group 1.8882900842547574e-05 data time  4.71 step time  0.90 forward time  0.30 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  33 | time: 499.11s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-,-,-,-, 0.26,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-, 0.13,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-, 0.19,-,-,-,-, 0.24,-,-,-, 0.19,-,-, 0.26,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.42,-,-,-,-,-, 0.25,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-, 0.50, 0.15,-,-,-,-,-,-,-,-, 0.26,-,-,-,-, 0.11,-,-,-,-,-,-,-,-,-,-,-, 0.20, 0.19,-,-,-, 0.27, 0.12,-,-,-,-,-,-,-,-, 0.34, 0.26,-,-,-,-,-, 0.17,-, 0.20,-,-,-,-,-,-, 0.13, 0.18,-,-,-, 0.17,-,-,-, 0.14,-, 0.21,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-, 0.17,-,-,-,-,-,-, 0.21,-,-,-,-, 0.24,-,-,-,-,-, 0.19, 0.11,-,-, 0.19,-,-, 0.14,-, 0.18,-,-, 0.16,-,-,-, 0.28, 0.17, 0.31,-,-,-, 0.31,-, 0.24,-,-,-,-,-,-, 0.11,-,-,-, 0.73,-, 0.19, 0.24, 0.07,-,-,-,-,-,-,-,-, 0.13,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-, 0.52, 0.22,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-, 0.29,-, 0.31,-,-, 0.24, 0.19,-,-, 0.13,-,-,-,-,-,-,-, 0.17,-,-, 0.12,-, 0.17,-,-,-, 0.22,-,-, 0.06,-,-,-,-,-, 0.24, 0.20, 0.10, 0.07,-, lr of zeroth group 1.6902064545046257e-05 data time  3.78 step time  1.16 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  34 | time: 502.30s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-,-, 0.15,-,-,-,-,-, 0.19,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.11,-, 0.33, 0.11,-,-,-,-,-,-,-,-, 0.26,-, 0.14,-,-,-,-,-,-,-,-,-, 0.33,-, 0.16,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-, 0.36,-,-,-, 0.26,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-, 0.14,-,-, 0.24,-,-,-, 0.27,-, 0.13, 0.22,-, 0.13,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-, 0.13,-,-,-,-,-,-,-,-, 0.21,-,-,-,-, 0.14,-,-,-,-, 0.48, 0.15, 0.11, 0.16,-,-,-,-,-,-,-, 0.13,-,-,-,-, 0.20, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.09,-,-,-,-, 0.15, 0.16,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-, 0.16,-, 0.13,-,-,-,-,-,-,-, 0.20, 0.15,-,-,-, 0.21,-,-, 0.25,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-, 0.38,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-,-,-, 0.11,-,-,-,-,-,-, 0.13,-,-,-, 0.16,-,-, 0.19,-,-,-,-,-,-,-,-,-, 0.33,-, 0.11,-,-,-, 0.10, 0.24,-, 0.12,-,-,-, 0.24,-,-,-,-,-, 0.12,-,-,-,-, 0.14,-,-, 0.14,-,-,-, 1.26,-, 0.18, 0.19,-,-, 0.18,-,-,-,-,-,-,-,-,-, 0.14, 0.45, 0.13,-, 0.19, 0.65,-, 0.17,-,-, 0.08,-, 0.30, 0.11,-,-, 0.18, 0.06, 0.19, 0.22, 0.51,-, 1.25,-, lr of zeroth group 1.4976545357900817e-05 data time  3.59 step time  1.02 forward time  0.34 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  35 | time: 516.11s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.39,-,-, 0.19,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-, 0.20,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15, 0.19,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-, 0.19,-,-,-, 0.19,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.34,-,-, 0.14,-,-,-, 0.24, 0.20,-, 0.38,-,-, 0.13,-,-,-,-, 0.19,-,-,-, 0.24, 0.21,-, 0.24,-,-,-,-, 0.16,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-, 0.15, 0.22,-,-,-,-, 0.30, 0.26,-,-,-,-,-,-, 0.39,-,-,-, 0.26,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-, 0.19,-, 0.20, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14, 0.14,-,-, 0.21,-, 0.20,-,-, 0.22,-,-,-,-,-,-, 0.14,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.11,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-, 0.23, 0.21,-,-, 0.25,-,-, 0.23,-, 0.21, 0.12,-,-,-,-,-,-,-, 0.08, 0.20,-, 0.12,-, 0.19,-, 0.09, 0.19, 0.27,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-, 0.24, 0.31,-,-,-,-,-,-,-,-, 0.31,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.45,-,-, 0.27,-,-,-,-,-,-,-,-, 0.08,-,-,-,-, 0.19,-,-,-, 0.20, 0.14, 0.19,-,-,-,-, 0.14,-, 0.19, 0.15, 0.10,-,-,-,-, 0.20,-,-,-, 0.11,-,-,-,-,-,-, 0.19, 0.15, 0.20,-,-,-, 0.15,-, 0.11,-, 0.19,-,-, 0.17,-,-, 0.08,-,-, 0.15, 0.25,-, 0.08,-, 0.17,-,-,-,-,-,-, lr of zeroth group 1.3119496529610197e-05 data time  3.61 step time  1.20 forward time  0.41 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  36 | time: 520.35s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-, 0.15,-,-, 0.11,-,-, 0.20,-,-, 0.22,-,-,-,-,-, 0.14,-,-,-, 0.17,-, 0.21,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26, 0.11,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-,-,-,-, 0.12,-,-, 0.10,-,-,-,-,-,-,-,-,-, 0.14, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-, 0.27,-,-,-, 0.32,-,-,-,-,-, 0.11,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-, 0.16,-, 0.15,-,-,-,-, 0.19,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-, 0.19,-,-,-,-,-,-, 0.22,-,-,-,-, 0.18, 0.19,-,-,-,-,-,-,-,-,-,-,-, 0.18,-, 0.27,-,-,-,-, 0.15,-,-, 0.20, 0.19,-, 0.18,-,-,-,-,-,-,-, 0.15,-,-,-,-,-, 0.15,-,-,-, 0.15,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-, 0.22,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.35,-, 0.16,-,-,-, 0.18,-,-,-,-,-,-,-,-, 0.14,-,-, 0.23,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-, 0.17,-,-, 0.16,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-, 0.17, 0.16,-,-, 0.21,-,-,-, 0.16,-,-,-, 0.22, 0.22,-, 0.26, 0.25,-,-, 0.28,-,-,-, 0.29,-, 0.28,-,-,-, 0.17,-,-, 0.17,-,-,-,-, 0.19,-, 0.17,-,-, 0.62, 0.21,-, 0.09,-, 0.77,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-, 0.20,-,-,-,-,-, 0.16, 0.09, 0.10,-, 0.11,-,-, 0.09,-, 0.22, 0.07,-,-, 0.15,-, 0.09, 6.72,-, 0.14, 0.06,-, 0.17,-, 0.49,-,-, lr of zeroth group 1.1343603586694372e-05 data time  3.93 step time  1.49 forward time  0.55 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  37 | time: 534.73s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-, 0.23,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-, 0.32,-, 0.23,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-, 0.11,-,-,-,-,-,-,-, 0.11,-,-,-,-,-,-,-,-,-, 0.26,-,-,-, 0.23,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-, 0.21,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.11,-, 0.24,-,-, 0.39, 0.19,-,-,-,-,-,-,-,-,-,-, 0.18,-,-, 0.11,-, 0.17,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-, 0.36,-, 0.28,-,-,-,-,-,-,-, 0.15, 0.26,-,-,-,-, 0.23, 0.10,-,-,-,-,-,-,-, 0.22,-, 0.20,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-, 0.45,-,-,-,-,-,-,-,-, 0.11,-,-, 0.20, 0.30, 0.63,-,-, 0.43,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-, 0.19,-,-, 0.13, 0.23, 0.16,-,-,-,-,-, 0.16,-, 0.23,-,-, 0.07,-, 0.21,-, 0.12,-,-,-, 0.09,-, 0.19,-, 0.17, 0.11, 0.23,-,-,-,-, 0.15,-, 0.11,-,-,-,-,-, 0.25,-, 0.22,-, 0.14,-, 0.13,-,-,-,-,-,-, 0.18, 0.10,-, 0.17, 0.12,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-, 0.14, 0.23, 0.14, 0.31, 0.23, 0.32,-,-,-, 0.19, 0.19, 0.25,-,-, 0.08,-, lr of zeroth group 9.660997678685216e-06 data time  3.27 step time  1.21 forward time  0.41 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  38 | time: 536.91s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-, 0.16,-,-, 0.40,-,-, 0.09, 0.18,-,-,-,-,-,-,-,-,-, 0.24, 0.52,-,-,-,-,-,-,-,-,-,-, 0.14, 0.17,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-, 0.30,-,-,-, 0.36,-,-,-,-, 0.21,-,-,-,-,-,-,-,-, 0.23,-, 0.34,-,-, 0.28,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-, 0.23,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-, 0.15,-, 0.19,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-, 0.31, 0.33,-,-,-,-, 0.19, 0.27,-, 0.21, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-, 0.15, 0.12,-, 0.11, 0.21,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-, 0.14,-, 0.19,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-, 0.20,-,-,-, 0.28,-,-,-, 0.23, 0.17,-,-,-, 0.21,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.09,-, 0.12,-,-,-,-,-,-,-, 0.28,-, 0.16,-,-,-, 0.19,-,-,-,-,-,-,-,-,-, 0.24,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-, 0.16,-,-, 0.19, 0.19,-,-,-,-,-, 0.20,-,-,-, 0.81, 0.16, 0.15,-, 0.22,-,-,-,-, 0.23,-,-, 0.60,-,-, 0.21, 0.18,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-, 0.20,-, 0.22,-,-,-, 0.11,-,-, 0.20,-,-,-, 0.15,-,-,-,-,-,-,-, 0.04,-,-,-,-, 0.29,-,-,-,-,-, 0.17,-,-,-,-, 0.19,-,-, 0.24, 0.13,-, 0.31, 0.30,-, 0.21, 0.08,-, 0.05, 0.19,-, lr of zeroth group 8.083172710074385e-06 data time  3.34 step time  1.19 forward time  0.41 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  39 | time: 534.56s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-, 0.17,-,-, 0.15,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-, 0.28,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-,-, 0.15,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-, 0.21,-,-, 0.15, 0.21,-,-,-,-, 0.36,-,-, 0.36, 0.20,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-, 0.17,-,-,-,-,-, 0.23, 0.37,-,-,-,-, 0.20,-,-, 0.35,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-, 0.18,-,-, 0.14,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-, 0.09,-, 0.18,-, 0.33,-, 0.16,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-, 0.17,-,-, 0.17, 0.29,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-, 0.13,-,-,-, 0.17,-,-,-,-,-,-, 0.14,-,-,-, 0.19, 0.10, 0.24, 0.16,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-, 0.17, 0.18,-,-,-,-,-,-,-, 0.17, 0.15,-,-,-, 0.27,-,-,-, 0.16,-, 0.12,-, 0.39, 0.21,-,-, 0.16,-,-,-,-,-,-, 0.16,-,-,-,-,-, 0.39,-,-,-,-,-,-,-,-,-, 0.11, 0.10, 0.22,-,-, 0.17, 0.29,-,-,-, 0.14, 0.17,-,-, 0.16, 0.30,-,-, 0.16,-,-, 0.17,-,-,-,-, 0.10, 0.17,-,-, 0.06,-,-,-, 0.14, 1.36, 0.37,-, lr of zeroth group 6.620906825290946e-06 data time  3.02 step time  1.22 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  40 | time: 540.15s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16, 0.27,-,-, 0.29,-,-,-,-,-,-, 0.41,-,-,-, 0.28,-, 0.22,-,-,-,-,-,-,-,-,-, 0.31,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33, 0.16,-,-,-,-, 0.18,-,-, 0.17,-,-, 0.19,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-, 0.25,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-, 0.32,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-, 0.22,-,-,-,-, 0.09,-,-,-, 0.21, 0.10, 0.18,-,-, 0.15,-,-,-,-,-,-, 0.12,-,-,-,-,-,-, 0.20, 0.17, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-, 0.22,-,-,-,-, 0.19,-,-,-,-, 0.16,-,-,-,-,-,-, 0.13,-,-,-,-,-, 0.19,-, 0.21,-,-,-,-,-,-,-,-,-,-,-, 0.21, 0.25,-,-,-, 0.18,-,-, 0.25,-,-, 0.19,-,-,-, 0.28,-,-, 0.18,-,-,-, 0.12, 0.13,-, 0.25,-,-,-,-, 0.11,-, 0.12,-, 0.22,-,-,-,-,-,-, 0.23,-, 0.27,-, 0.16, 0.17,-,-,-,-,-, 0.14, 0.11,-,-, 0.17, 0.12,-,-,-,-, 0.26, 0.32, 0.24, 0.27,-,-,-,-, 0.15,-, 0.13,-,-,-,-, 0.26,-, 0.19, 0.27,-, 0.12, 0.19,-, 0.15,-, 0.20, 0.07, 0.08,-, 0.24,-, 0.12, 0.33,-, lr of zeroth group 5.284188783046082e-06 data time  4.63 step time  1.13 forward time  0.46 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  41 | time: 547.72s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-, 0.25,-,-,-, 0.23,-,-, 0.22,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-, 0.18,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-, 0.17, 0.24,-,-,-,-,-, 0.21,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-, 0.53,-,-,-,-,-, 0.25,-, 0.41,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-, 0.19,-,-,-,-, 0.23,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19, 0.13,-,-,-,-, 0.18,-,-, 0.26,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-, 0.21,-,-,-,-,-,-, 0.17, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-, 0.19, 0.31, 0.17,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-, 0.18,-,-,-,-,-,-,-,-, 0.43, 0.21, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-, 0.36,-,-,-, 0.17, 0.15,-,-, 0.23, 0.25,-,-,-,-,-,-, 0.13,-,-, 0.15, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.36,-,-, 0.10, 0.18, 0.17, 0.24,-,-, 0.11,-,-,-,-,-,-,-,-,-, 0.13, 0.18, 0.15,-, 1.02,-,-, 0.11,-, 0.06, 0.18, 0.24,-, 0.18,-,-, 0.13,-,-, 0.35,-, 0.10,-,-, 0.17,-,-, 0.18, 0.16,-, 0.23,-,-, 0.11, 0.09, 0.08, 0.14,-,-, lr of zeroth group 4.082149722982584e-06 data time  4.93 step time  1.33 forward time  0.45 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  42 | time: 552.96s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-, 0.24,-,-,-,-,-,-,-,-,-,-, 0.17,-, 0.27,-,-,-,-,-,-,-,-,-,-, 0.21,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-, 0.41,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-, 0.25,-,-,-,-,-, 0.17,-, 0.28,-,-,-,-,-,-,-,-, 0.22,-,-,-, 0.15,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-, 0.14,-, 0.11,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-, 0.12,-,-,-, 0.16,-,-, 0.26,-, 0.25, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.51, 0.20,-, 0.26,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-, 0.16,-, 0.15,-,-, 0.12,-,-, 0.21,-,-,-, 0.25,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-, 0.26,-,-,-,-,-,-, 0.31, 0.13,-,-,-,-,-,-,-,-,-,-,-,-, 0.39, 0.22,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-, 0.16,-,-,-,-,-, 0.18,-,-,-, 0.29, 0.15,-,-,-,-,-, 0.22, 0.15,-,-,-,-,-,-, 0.13,-,-, 0.23,-,-,-, 0.13,-, 0.28,-,-, 0.12, 0.13,-, 0.25,-, 0.22,-,-,-,-,-,-,-,-,-, 0.16,-,-,-, 0.12,-,-, 0.12,-,-,-,-,-, 0.14,-,-, 0.17, 0.13,-,-, 0.16,-,-,-,-,-,-, 0.22,-,-,-, 0.30,-, 0.33, 0.16,-,-,-, 0.25,-,-, 0.37, 0.17,-,-,-, 0.21,-,-,-,-,-,-, 0.17,-, 0.22,-,-,-, 0.10, 0.18,-,-,-, 0.12, 0.09,-, 0.13,-, 0.07, 0.17, 0.22,-, 0.05,-,-, lr of zeroth group 3.0230007907322526e-06 data time  3.96 step time  1.18 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  43 | time: 536.37s | mean loss  0.20 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12, 0.22,-,-,-,-,-,-,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-,-,-,-, 0.19,-,-, 0.16,-,-,-,-,-,-,-,-,-,-, 0.30,-,-, 0.12,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-, 0.21,-, 0.28, 0.23,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-, 0.31,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-,-,-, 0.39,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-, 0.21, 0.16, 0.19,-,-, 0.15, 0.16,-, 0.15,-,-,-,-,-,-,-, 0.22,-, 0.19,-, 0.15,-,-,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-, 0.33,-, 0.16,-,-,-,-,-,-, 0.18,-,-,-,-,-, 0.42,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22, 0.14, 0.16, 0.14,-, 0.47,-,-,-, 0.29, 0.22,-,-,-,-,-,-,-,-, 0.26, 0.11,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-, 0.11,-,-, 0.19, 0.19,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-, 0.11,-, 0.25,-,-,-, 0.14,-,-,-,-,-, 0.16, 0.14,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-, 0.18,-,-,-, 0.17,-,-,-,-,-,-,-, 0.10,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-, 0.13,-,-, 0.23, 0.09, 0.19,-,-,-,-,-,-,-, 0.18, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14, 0.12,-, 0.10, 0.13,-,-,-,-,-,-,-,-, 0.24,-, 0.28, 0.07,-, 0.19,-,-,-,-,-,-, 0.13, 0.16,-,-,-, 0.20, 0.20,-,-,-, 0.21,-,-, 0.09,-,-, 0.13,-,-, 0.23,-, 0.10,-,-, 0.31, 0.16,-, 0.18,-,-, 0.40,-, 0.25, 0.10,-,-,-, 0.09,-, 0.14, 0.16,-, 0.44,-, lr of zeroth group 2.11397704746526e-06 data time  3.90 step time  1.38 forward time  0.46 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  44 | time: 535.32s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-, 0.32,-,-,-,-,-,-, 0.14,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-, 0.11,-,-, 0.30,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-, 0.19,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25, 0.20,-,-,-, 0.17,-, 0.30,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-, 0.10,-, 0.23,-,-,-,-, 0.18,-,-,-, 0.16, 0.18,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20, 0.27,-, 0.30,-,-,-,-,-,-,-,-, 0.25,-,-, 0.23,-,-, 0.29, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17, 0.12,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-, 0.20,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24, 0.41,-,-,-,-,-,-, 0.35,-,-,-,-,-, 0.62,-,-, 0.24,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-, 0.43,-,-,-,-, 0.31,-,-,-, 0.21, 0.11,-, 0.69,-,-, 0.17,-,-,-,-,-,-,-, 0.21, 0.57,-, 0.18,-,-,-, 0.40,-,-, 1.34,-, 0.18,-,-, 0.09, 0.16,-, 0.22,-,-,-,-,-,-, 0.18,-,-, 0.18,-, 0.27, 0.16, 0.20,-, 0.12, 0.14,-,-,-,-, 0.13,-,-, 0.20,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-, 0.27, 0.10,-,-,-, 0.06,-, 0.01, 0.13, 0.89,-, lr of zeroth group 1.361288047086051e-06 data time  3.85 step time  1.53 forward time  0.47 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  45 | time: 531.76s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-, 0.14,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-, 0.14, 0.17,-,-, 0.14,-,-,-,-, 0.23, 0.16,-,-, 0.30, 0.30,-,-,-,-,-,-,-, 0.12, 0.21, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.11,-,-,-, 0.17,-,-,-,-, 0.25,-,-,-,-,-,-, 0.21,-,-,-,-, 0.19,-,-, 0.21,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-, 0.19,-, 0.30, 0.11,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-, 0.13, 0.33,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-, 0.11,-,-,-, 0.17,-, 0.29, 0.16,-, 0.35,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-, 0.14,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.30,-,-, 0.11,-, 0.21,-,-,-,-,-,-,-, 0.24,-,-, 0.22,-,-,-, 0.22,-,-,-,-,-,-,-, 0.16,-, 1.11,-,-,-,-,-,-,-, 0.13, 0.15,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22, 0.17,-,-,-,-,-,-, 0.20,-,-, 0.16, 0.51, 0.16,-,-,-,-, 0.16, 0.22,-,-,-, 0.21,-,-,-,-, 0.60, 0.15,-,-, 0.26,-,-,-,-,-,-,-,-,-,-, 0.14, 0.13,-,-, 0.19,-,-,-,-, 0.18,-, 0.14,-,-,-,-,-, 0.29, 0.15,-,-, 0.21,-,-,-,-, 0.17,-, 0.13,-,-,-, 0.15,-,-, 0.36, 0.08, 0.13,-,-,-, 0.06,-, 0.11, 0.31,-,-, 0.12, 0.05, 0.05,-, lr of zeroth group 7.700754186838382e-07 data time  3.91 step time  1.49 forward time  0.49 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  46 | time: 536.84s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-, 0.23, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.36,-, 0.10,-, 0.11, 0.14, 0.18,-, 0.32,-,-,-,-,-,-, 0.27,-,-,-,-,-,-, 0.29,-,-,-,-, 0.12, 0.18,-,-,-,-,-,-,-,-, 0.26, 0.14,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-, 0.25,-,-,-,-, 0.33,-,-,-, 0.11, 0.27,-, 0.21,-,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-, 0.17,-,-, 0.21,-,-,-,-,-, 0.16, 0.22, 0.31,-,-, 0.26,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-, 0.16,-,-, 0.29, 0.38,-,-, 0.23,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-, 0.22,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-, 0.36,-, 0.18, 0.25,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-, 0.16, 0.19,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-, 0.28,-,-, 0.25,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20, 0.13,-, 0.13,-,-,-,-,-,-,-, 0.17, 0.15,-, 0.13,-,-, 0.21, 0.20, 0.24,-, 0.10,-,-, 0.19,-, 0.18, 0.18,-,-, 0.13, 0.19,-,-,-, 0.16,-,-,-, 0.19,-,-,-,-,-,-, 0.43,-, 0.18,-, 0.12,-,-,-,-,-,-,-,-, 0.34,-, 0.18, 0.09, 0.19,-, lr of zeroth group 3.4437774399231904e-07 data time  4.05 step time  1.02 forward time  0.33 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  47 | time: 528.13s | mean loss  0.20 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.12,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-, 0.11,-,-,-, 0.21, 0.18,-,-, 0.21,-,-,-,-,-,-,-,-, 0.21,-,-,-, 0.26,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-, 0.25,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-, 0.15,-,-,-,-,-,-,-,-,-, 0.24,-,-, 0.15,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-, 0.20,-,-,-, 0.19, 0.10,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-, 0.22,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-, 0.20,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-, 0.37, 0.16,-,-, 0.18, 0.23, 0.10,-,-, 0.19, 0.22,-,-,-, 0.12,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-,-,-,-, 0.19, 0.20,-, 0.16,-,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-, 0.11,-,-,-, 0.15, 0.22,-,-,-,-,-,-, 0.41,-,-,-, 0.31, 0.09, 0.17, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-, 0.15, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13, 0.21,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-, 0.16,-,-, 0.32,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.32,-,-, 0.27, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.33,-,-, 0.22,-,-,-,-,-,-, 0.41,-, 0.17,-,-, 0.26,-, 0.12,-, 0.14,-,-,-,-,-,-,-,-,-, 0.05,-,-,-,-, 0.16, 0.14,-, 0.06, 0.22,-,-, 0.22, 0.16,-, 0.26,-, 0.19,-, 0.21, 0.11,-, 0.23, 0.11, 0.17,-,-,-, lr of zeroth group 8.710296978139965e-08 data time  5.06 step time  0.94 forward time  0.30 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  48 | time: 530.92s | mean loss  0.21 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21, 0.22,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-, 0.12,-,-,-, 0.23,-,-, 0.25,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-,-, 0.30,-,-, 0.38, 0.20,-,-,-,-, 0.10,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-, 0.16,-, 0.22, 0.22,-,-,-,-,-,-,-,-,-,-,-,-, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-, 0.59,-,-,-,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27, 0.22,-,-,-,-,-,-, 0.11,-,-,-,-, 0.25,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-, 0.17,-, 0.23, 0.19,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.10,-,-,-,-,-,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-,-, 0.42,-, 0.25,-,-,-,-,-,-, 0.23,-,-,-, 0.36,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-, 0.17,-,-,-,-,-,-, 0.35,-,-,-,-,-, 0.13,-, 0.17,-,-,-,-,-,-,-,-,-, 0.21,-,-,-, 0.49,-,-,-,-,-,-,-,-,-, 0.29, 0.20,-,-,-,-,-,-,-,-,-, 0.23,-,-, 0.13,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17, 0.14,-, 0.14,-, 0.15, 0.17,-,-,-,-,-,-,-, 0.06, 0.19, 0.34, 0.19,-, 0.14,-,-,-, 0.13,-,-,-, 0.12,-,-, 0.20,-,-,-,-, 0.15, 0.42,-, 0.18,-, 0.34,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-, 0.19,-,-, 0.22,-,-,-, 0.26,-,-,-,-,-, 0.15,-,-,-,-, 0.16,-, 0.30,-, 0.76, 0.12,-, 0.31,-, 0.14, 0.20,-,-,-, 5.35,-,-, 0.15,-, 0.26, 0.11,-, 0.11, 0.03, 0.22,-, 0.07,-, lr of zeroth group 8.543632132318457e-12 data time  3.54 step time  0.89 forward time  0.31 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  49 | time: 538.62s | mean loss  0.22 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-, 0.23,-,-,-,-,-, 0.17, 0.21,-, 0.32,-,-,-,-,-, 0.21,-, 0.18,-,-,-,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-, 0.26,-, 0.10, 0.25,-,-, 0.25,-,-, 0.35,-,-,-,-, 0.14,-,-, 0.27,-, 0.15,-,-,-,-,-,-, 0.30,-, 0.32, 0.18,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-, 0.14,-,-,-, 0.32,-,-, 0.24,-,-, 0.15,-,-,-,-, 0.26, 0.17,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-, 0.21,-, 0.25,-, 0.24,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.11, 0.22,-,-, 0.25,-,-, 0.19,-, 0.33, 0.22,-,-,-,-,-,-,-,-,-,-,-,-, 0.35,-,-, 0.25,-,-,-,-,-, 0.15,-,-,-,-,-, 0.20, 0.32,-,-,-, 0.23,-,-, 0.16,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-, 0.21,-,-,-,-,-,-,-, 0.21,-,-,-, 0.44,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-, 0.37,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-, 0.10,-,-,-,-,-, 0.19,-,-, 0.24,-,-,-,-,-, 0.13,-,-,-, 0.15,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-,-, 0.43,-,-,-,-,-,-,-,-, 0.74,-, 0.15,-,-, 0.54,-,-,-,-,-,-, 0.13,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-, 0.27,-, 0.20, 0.21,-,-,-,-,-,-,-, 0.27,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-, 0.18, 0.19,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-, 0.16,-,-,-, 0.30,-, 0.14,-,-,-,-, 0.12,-,-, 0.30,-,-,-,-,-,-,-,-, 0.23,-, 0.15,-,-,-,-,-,-,-, 0.19, 0.09,-,-,-, 0.06, 0.33,-, lr of zeroth group 8.36894087877016e-08 data time  4.32 step time  1.43 forward time  0.47 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  50 | time: 539.31s | mean loss  0.20 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-, 0.17,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-, 0.12,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-,-,-,-,-,-, 0.09,-,-,-,-,-, 0.18,-,-,-,-,-, 0.22,-, 0.23, 0.09, 0.19, 0.22,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-,-,-,-,-,-, 0.18, 0.19,-,-,-,-,-,-,-, 0.11,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.15, 0.15,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-, 0.14,-,-, 0.20, 0.22,-,-,-, 0.16,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.29,-, 0.43, 0.14,-,-,-,-,-, 0.17,-, 0.22,-,-,-,-,-, 0.30,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-, 0.13, 0.19,-,-,-,-,-,-,-, 0.26,-,-, 0.20,-,-,-, 0.22, 0.22, 0.21,-,-,-,-,-, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-, 0.29,-,-,-,-,-,-,-,-,-, 0.16,-,-,-,-,-,-,-,-,-, 0.35,-,-,-,-,-,-,-, 0.15, 0.22,-,-,-,-,-,-,-,-,-, 0.34,-, 0.13,-,-,-,-,-,-,-,-,-, 0.12,-, 0.30,-,-, 0.16,-,-,-,-,-,-, 0.21, 0.17,-,-, 0.18, 0.24,-,-,-,-, 0.17,-, 0.28,-,-,-,-,-,-,-, 0.17, 0.62,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-, 0.12,-, 0.17,-,-,-,-,-, 0.14,-,-,-, 0.14, 0.13,-,-,-,-, 0.20,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.20,-,-,-, 0.17,-, 0.16, 0.16, 0.52, 0.15,-,-, 0.22,-, 0.25,-,-, 0.14,-,-, 0.13,-, 0.24,-,-,-,-,-, 0.07, 0.16, 0.21,-, 0.04, 0.31,-, 0.02, 0.01,-, lr of zeroth group 3.3757394008781526e-07 data time  3.94 step time  1.16 forward time  0.40 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
DONE
Finished at Tue May  7 04:51:37 PM CEST 2024
