FOUND eth0 INTERFACE
Worker starting at: 20240502-135137
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x778cb45dae60>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 2, 'relu': False, 'sigmoid': True, 'sin': False, 'state_offset': 3.036257689580109, 'state_scale': 17.48895333036381, 'tanh': False, 'test': False, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': False, 'use_res_connection': False, 'width_hidden': 27, 'dropout_p': 0.8218737153434615}}, 'get_batch_method': <function get_batch at 0x778cb45a1cf0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 100 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x778cb45dae60>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 2, 'relu': False, 'sigmoid': True, 'sin': False, 'state_offset': 3.036257689580109, 'state_scale': 17.48895333036381, 'tanh': False, 'test': False, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': False, 'use_res_connection': False, 'width_hidden': 27, 'dropout_p': 0.8218737153434615}}, 'get_batch_method': <function get_batch at 0x778cb45a1cf0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (None, 14)  and nout 14
Using a Transformer with 13.17 M parameters
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
-----------------------------------------------------------------------------------------
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.2119085788726807, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.4711939096450806, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.4571651220321655, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5349913239479065, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.4527666568756104, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 6.896817684173584, nan, nan, nan, nan, nan, nan, nan, nan, 3.7110137939453125, nan, nan, nan]
| end of epoch   1 | time: 367.85s | mean loss  0.14 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 1.21,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 1.47,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 1.46,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.53,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 1.45,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 6.90,-,-,-,-,-,-,-,-, 3.71,-,-,-, lr of zeroth group 1.2458333333333336e-05 data time 329.39 step time  1.09 forward time  0.25 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
All batches have NaN in loss. Target NaN False output NaN True
-----------------------------------------------------------------------------------------
0
Using cpu:0 device
init dist
Not using distributed
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 4, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x778cb1452a70>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 4, 'relu': True, 'sigmoid': True, 'sin': True, 'state_offset': 4.357515953151063, 'state_scale': 19.631126249218653, 'tanh': True, 'test': False, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': True, 'width_hidden': 58, 'dropout_p': 0.13120933124874545}}, 'get_batch_method': <function get_batch at 0x778cb45a1cf0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized data loader with 100 steps and 4 batch size
DataLoader.__dict__ {'num_features': 14, 'num_steps': 100, 'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x778cb1452a70>, 'seq_len_maximum': None, 'device': 'cpu:0', 'get_batch_kwargs': {'epoch_count': 0, 'hyperparameters': {'env_name': 'FullNNEnv', 'num_hidden': 4, 'relu': True, 'sigmoid': True, 'sin': True, 'state_offset': 4.357515953151063, 'state_scale': 19.631126249218653, 'tanh': True, 'test': False, 'use_bias': False, 'use_dropout': True, 'use_layer_norm': True, 'use_res_connection': True, 'width_hidden': 58, 'dropout_p': 0.13120933124874545}}, 'get_batch_method': <function get_batch at 0x778cb45a1cf0>, 'model': None, 'epoch': 0, 'test_loader': False}
Initialized decoder for standard with (None, 14)  and nout 14
Using a Transformer with 13.17 M parameters
-----------------------------------------------------------------------------------------
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.7001383304595947, nan, nan, nan, nan, nan, 0.533921480178833, nan, nan, nan, 0.5834142565727234, nan, 0.5223597884178162, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.49585533142089844, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.4509493112564087, nan, nan, nan, nan, nan, 0.4921305775642395, nan, nan, nan, nan, 0.5743005275726318, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.4396950900554657, nan, nan, 0.6310766339302063, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.6658177971839905, nan, nan, nan, nan, nan, nan, 0.6336109042167664, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.4527985751628876, nan, nan, nan, 0.5052080750465393, nan, nan, nan, 0.5883879661560059, nan, nan, nan, nan, nan, nan, 0.43745100498199463, nan, nan, nan, nan, nan, nan, nan, nan, 0.6920738220214844, nan, nan, nan, nan, nan, nan, nan, nan, 0.5274878144264221, nan, nan, 0.5375701189041138, nan, nan, 0.4945995509624481, nan, nan, nan, 0.39076727628707886, 0.6137381792068481, nan, nan, nan, nan, nan, nan, nan, nan, 0.44269049167633057, 0.5861111879348755, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.4752045273780823, nan, nan, nan, nan, nan, nan, 0.5926207304000854, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5152102112770081, nan, nan, nan, nan, nan, nan, nan, nan, 0.557067334651947, nan, nan, nan, nan, 0.42813101410865784, nan, nan, nan, 0.5874015092849731, 0.4255918264389038, 0.6508727073669434, nan, nan, 0.4173673093318939, nan, 0.46957823634147644, nan, nan, nan, nan, nan, nan, nan, 0.5362347960472107, 0.5970802903175354, 0.3377016484737396, nan, nan, nan, nan, 0.5054272413253784, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5153726935386658, nan, nan, nan, 0.4627036452293396, nan, nan, nan, nan, nan, nan, 0.48425036668777466, nan, nan, nan, nan, nan, 0.5224728584289551, 0.5385898351669312, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.7131768465042114, nan, nan, nan, nan, nan, nan, nan, 0.4589066505432129, nan, nan, nan, nan, nan, nan, nan, nan, 0.30351153016090393, nan, 0.5009058117866516, nan, 0.6487493515014648, nan, 0.6251440048217773, 0.6690454483032227, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.41099005937576294, 0.5098795890808105, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5693552494049072, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.6424176692962646, 0.509200930595398, nan, 0.36538296937942505, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.7850847244262695, nan, nan, nan, nan, nan, nan, nan, nan, 0.56367027759552, 0.543897271156311, nan, 0.35909849405288696, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.43621236085891724, 0.520253598690033, nan, nan, nan, nan, nan, nan, nan, nan, 0.5561017990112305, nan, nan, nan, 0.6189063191413879, nan, nan, nan, nan, nan, nan, nan, nan, 0.42294806241989136, 0.5169293880462646, nan, nan, nan, nan, 0.451593279838562, nan, 0.7079014778137207, nan, nan, nan, 0.7190178632736206, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.6449096202850342, 0.5521986484527588, 0.7915357351303101, 0.6252127289772034, 0.4346257448196411, 0.3235790729522705, nan, nan, 0.5103588700294495, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5856016278266907, nan, nan, nan, nan, nan, nan, 0.3560241162776947, 1.242279052734375, nan, 0.5134214162826538, nan, 0.5493995547294617, 0.29217857122421265, nan, nan, 0.4144779443740845, 0.5924648642539978, 0.7037628293037415, nan, 0.5487940311431885, nan, nan, 0.9248831272125244, 0.6958127617835999, nan, 0.3141958713531494, nan, 0.3930007517337799, nan, nan, 0.34733128547668457, nan]
| end of epoch   1 | time: 609.88s | mean loss  0.56 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.70,-,-,-,-,-, 0.53,-,-,-, 0.58,-, 0.52,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.50,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.45,-,-,-,-,-, 0.49,-,-,-,-, 0.57,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.44,-,-, 0.63,-,-,-,-,-,-,-,-,-, 0.67,-,-,-,-,-,-, 0.63,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.45,-,-,-, 0.51,-,-,-, 0.59,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-,-, 0.69,-,-,-,-,-,-,-,-, 0.53,-,-, 0.54,-,-, 0.49,-,-,-, 0.39, 0.61,-,-,-,-,-,-,-,-, 0.44, 0.59,-,-,-,-,-,-,-,-,-, 0.48,-,-,-,-,-,-, 0.59,-,-,-,-,-,-,-,-,-,-,-, 0.52,-,-,-,-,-,-,-,-, 0.56,-,-,-,-, 0.43,-,-,-, 0.59, 0.43, 0.65,-,-, 0.42,-, 0.47,-,-,-,-,-,-,-, 0.54, 0.60, 0.34,-,-,-,-, 0.51,-,-,-,-,-,-,-,-,-, 0.52,-,-,-, 0.46,-,-,-,-,-,-, 0.48,-,-,-,-,-, 0.52, 0.54,-,-,-,-,-,-,-,-,-,-,-,-, 0.71,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-, 0.30,-, 0.50,-, 0.65,-, 0.63, 0.67,-,-,-,-,-,-,-,-,-,-, 0.41, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.57,-,-,-,-,-,-,-,-,-,-,-, 0.64, 0.51,-, 0.37,-,-,-,-,-,-,-,-,-, 0.79,-,-,-,-,-,-,-,-, 0.56, 0.54,-, 0.36,-,-,-,-,-,-,-,-,-,-, 0.44, 0.52,-,-,-,-,-,-,-,-, 0.56,-,-,-, 0.62,-,-,-,-,-,-,-,-, 0.42, 0.52,-,-,-,-, 0.45,-, 0.71,-,-,-, 0.72,-,-,-,-,-,-,-,-,-,-,-, 0.64, 0.55, 0.79, 0.63, 0.43, 0.32,-,-, 0.51,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.59,-,-,-,-,-,-, 0.36, 1.24,-, 0.51,-, 0.55, 0.29,-,-, 0.41, 0.59, 0.70,-, 0.55,-,-, 0.92, 0.70,-, 0.31,-, 0.39,-,-, 0.35,-, lr of zeroth group 1.2458333333333336e-05 data time  5.31 step time  0.82 forward time  0.27 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.46226966381073, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5576242208480835, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.6380035877227783, nan, 0.5705095529556274, nan, 0.4148709177970886, nan, nan, 0.46425408124923706, nan, 0.21692895889282227, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.44514280557632446, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5385887622833252, nan, 0.4315759241580963, nan, nan, nan, nan, 0.44417697191238403, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.480363667011261, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5359252095222473, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5359508395195007, nan, nan, nan, nan, nan, nan, 0.35937345027923584, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.45748937129974365, nan, 0.339535117149353, nan, nan, nan, nan, nan, nan, 0.5705831050872803, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5546393394470215, nan, 0.6160845756530762, nan, nan, 0.49461618065834045, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.38261592388153076, nan, nan, nan, 0.4626169800758362, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.45688769221305847, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.40986740589141846, nan, nan, nan, nan, nan, nan, 0.6343714594841003, nan, nan, nan, nan, nan, nan, 0.4734210968017578, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5110785961151123, nan, nan, 0.4410375952720642, nan, 0.5438505411148071, 0.5357731580734253, nan, 0.7049712538719177, 0.5293765068054199, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5314076542854309, 0.5152031183242798, nan, nan, nan, nan, nan, nan, nan, 0.7193374037742615, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.47083598375320435, nan, nan, nan, nan, nan, nan, nan, nan, 0.43651866912841797, nan, nan, 0.5503401756286621, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5238321423530579, 0.4434860348701477, nan, nan, 0.4861704111099243, nan, nan, nan, nan, nan, nan, nan, 0.6150929927825928, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.43044883012771606, nan, nan, nan, 0.6040140390396118, 0.572982907295227, nan, nan, nan, 0.6008597016334534, nan, nan, nan, 0.5074509382247925, nan, nan, nan, nan, 0.6614291071891785, nan, nan, nan, nan, nan, nan, 0.5282070636749268, nan, nan, 0.4054797887802124, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5064808130264282, nan, nan, 0.38144785165786743, nan, nan, nan, nan, nan, nan, nan, nan, 0.3542937636375427, nan, nan, nan, nan, nan, 0.49055415391921997, nan, 0.3972751498222351, nan, nan, nan, 0.5037594437599182, 0.4257431626319885, nan, nan, nan, 0.2789948582649231, nan, nan, 0.5062078237533569, nan, nan, 0.36819887161254883, 0.5941463708877563, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.31187042593955994, 0.49771848320961, nan, nan, nan, nan, 0.5229749083518982, nan, 0.3057600259780884, nan, nan, 0.5057713985443115, nan, nan, 0.4210416376590729, nan, nan, nan, nan, nan, nan, 0.26233571767807007, nan, nan, nan, 0.599789023399353, nan, nan, nan, nan, 0.4651619493961334, nan, nan, 0.48812010884284973, 0.5676875710487366, 0.3622478246688843, 0.3746327757835388, nan, 0.5236442685127258, nan, 0.3654405474662781, 0.4802345335483551, nan, nan, nan, nan, nan, 0.5661380290985107, 0.3429039716720581, nan, 0.5812519192695618, nan, nan, nan, nan, nan, nan, nan, 0.45236486196517944, 0.734222948551178, 0.3941159248352051, 0.5594094395637512, 0.471706748008728, nan, nan, nan]
| end of epoch   2 | time: 612.69s | mean loss  0.53 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-,-, 0.56,-,-,-,-,-,-,-,-,-, 0.64,-, 0.57,-, 0.41,-,-, 0.46,-, 0.22,-,-,-,-,-,-,-,-,-, 0.45,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.54,-, 0.43,-,-,-,-, 0.44,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.48,-,-,-,-,-,-,-,-,-,-, 0.54,-,-,-,-,-,-,-,-,-,-,-, 0.54,-,-,-,-,-,-, 0.36,-,-,-,-,-,-,-,-,-, 0.46,-, 0.34,-,-,-,-,-,-, 0.57,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.55,-, 0.62,-,-, 0.49,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.38,-,-,-, 0.46,-,-,-,-,-,-,-,-,-, 0.46,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.41,-,-,-,-,-,-, 0.63,-,-,-,-,-,-, 0.47,-,-,-,-,-,-,-,-,-,-,-,-, 0.51,-,-, 0.44,-, 0.54, 0.54,-, 0.70, 0.53,-,-,-,-,-,-,-,-,-,-,-, 0.53, 0.52,-,-,-,-,-,-,-, 0.72,-,-,-,-,-,-,-,-,-,-,-, 0.47,-,-,-,-,-,-,-,-, 0.44,-,-, 0.55,-,-,-,-,-,-,-,-,-,-,-,-, 0.52, 0.44,-,-, 0.49,-,-,-,-,-,-,-, 0.62,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.43,-,-,-, 0.60, 0.57,-,-,-, 0.60,-,-,-, 0.51,-,-,-,-, 0.66,-,-,-,-,-,-, 0.53,-,-, 0.41,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.51,-,-, 0.38,-,-,-,-,-,-,-,-, 0.35,-,-,-,-,-, 0.49,-, 0.40,-,-,-, 0.50, 0.43,-,-,-, 0.28,-,-, 0.51,-,-, 0.37, 0.59,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.31, 0.50,-,-,-,-, 0.52,-, 0.31,-,-, 0.51,-,-, 0.42,-,-,-,-,-,-, 0.26,-,-,-, 0.60,-,-,-,-, 0.47,-,-, 0.49, 0.57, 0.36, 0.37,-, 0.52,-, 0.37, 0.48,-,-,-,-,-, 0.57, 0.34,-, 0.58,-,-,-,-,-,-,-, 0.45, 0.73, 0.39, 0.56, 0.47,-,-,-, lr of zeroth group 1.6625000000000004e-05 data time  5.29 step time  0.92 forward time  0.31 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.22007636725902557, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.442568302154541, nan, nan, nan, nan, nan, nan, nan, 0.24406549334526062, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.20736482739448547, 0.5370020270347595, 0.3732483685016632, 0.1687859147787094, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.342325359582901, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.235891342163086, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.23872949182987213, nan, nan, nan, nan, nan, nan, 0.7799267768859863, nan, nan, nan, 0.19204959273338318, nan, nan, 0.15346510708332062, nan, nan, nan, nan, 0.2266465127468109, nan, nan, 0.2898103892803192, nan, 0.4730408191680908, nan, nan, nan, nan, nan, 0.32194799184799194, nan, nan, 0.2918868362903595, nan, nan, nan, 0.2521679401397705, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.408520370721817, 0.44194158911705017, nan, nan, nan, nan, nan, nan, 0.33929234743118286, 0.30725863575935364, nan, 0.31272459030151367, nan, nan, nan, nan, nan, nan, 2.415283203125, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.24277079105377197, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.16261792182922363, nan, nan, nan, 0.1953105330467224, 0.26867350935935974, nan, nan, 0.3851569592952728, nan, nan, nan, 0.2205357402563095, 0.1903824657201767, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.3580623269081116, nan, nan, nan, 0.5718890428543091, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.44778144359588623, nan, 0.26253968477249146, nan, nan, nan, nan, nan, nan, 0.3307488262653351, nan, nan, nan, nan, nan, nan, nan, nan, 0.19790542125701904, nan, nan, nan, 0.4381081163883209, nan, nan, nan, nan, 0.2614714503288269, nan, nan, nan, nan, nan, 0.5305994749069214, 0.14344990253448486, nan, nan, nan, nan, nan, nan, 0.40419644117355347, 0.35072481632232666, 0.22999230027198792, nan, nan, 0.4236041307449341, nan, nan, nan, 0.17851939797401428, nan, nan, nan, nan, nan, 0.3028811514377594, nan, nan, nan, nan, nan, 0.30202630162239075, nan, nan, nan, nan, nan, 0.2523604929447174, nan, 0.3588337302207947, nan, nan, nan, nan, nan, 0.2874760925769806, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.2758840322494507, nan, 0.2576776444911957, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.24565157294273376, nan, nan, nan, 0.37221381068229675, nan, nan, nan, nan, nan, nan, nan, 0.14039704203605652, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.24161815643310547, nan, 0.22649595141410828, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.2070925086736679, nan, 0.35612303018569946, nan, nan, nan, 0.20671136677265167, nan, nan, 0.3457309603691101, nan, 0.31119483709335327, nan, nan, nan, nan, nan, 0.42724719643592834, nan, nan, nan, 0.3502920866012573, nan, 0.40731167793273926, nan, 0.24392563104629517, nan, nan, nan, nan, 0.25896701216697693, nan, nan, nan, nan, 0.32411032915115356, nan, 0.36517950892448425, nan, nan, nan, 0.3289722800254822, nan, nan, nan, 0.3876977562904358, nan, nan, nan, nan, nan, 0.243147611618042, nan, 0.3448939323425293, 0.3067755401134491, 0.2768116891384125, nan, 0.41237616539001465, 0.20833657681941986, nan, nan, 0.32371652126312256, 0.3841691017150879, 0.35837212204933167, nan, nan, nan, nan, nan, 0.3493346869945526, nan, nan, nan, nan, nan, 0.42386332154273987, nan, 0.19370736181735992, nan, nan, nan, nan, 0.3195803463459015, 0.26738643646240234, 0.3999534845352173, nan, nan, 0.07356502115726471, 0.06747967004776001, nan]
| end of epoch   3 | time: 630.38s | mean loss  0.33 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.22,-,-,-,-,-,-,-,-,-,-, 0.44,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21, 0.54, 0.37, 0.17,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.34,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 1.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-, 0.78,-,-,-, 0.19,-,-, 0.15,-,-,-,-, 0.23,-,-, 0.29,-, 0.47,-,-,-,-,-, 0.32,-,-, 0.29,-,-,-, 0.25,-,-,-,-,-,-,-,-,-, 0.41, 0.44,-,-,-,-,-,-, 0.34, 0.31,-, 0.31,-,-,-,-,-,-, 2.42,-,-,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.16,-,-,-, 0.20, 0.27,-,-, 0.39,-,-,-, 0.22, 0.19,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.36,-,-,-, 0.57,-,-,-,-,-,-,-,-,-,-,-, 0.45,-, 0.26,-,-,-,-,-,-, 0.33,-,-,-,-,-,-,-,-, 0.20,-,-,-, 0.44,-,-,-,-, 0.26,-,-,-,-,-, 0.53, 0.14,-,-,-,-,-,-, 0.40, 0.35, 0.23,-,-, 0.42,-,-,-, 0.18,-,-,-,-,-, 0.30,-,-,-,-,-, 0.30,-,-,-,-,-, 0.25,-, 0.36,-,-,-,-,-, 0.29,-,-,-,-,-,-,-,-,-, 0.28,-, 0.26,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.25,-,-,-, 0.37,-,-,-,-,-,-,-, 0.14,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.24,-, 0.23,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.21,-, 0.36,-,-,-, 0.21,-,-, 0.35,-, 0.31,-,-,-,-,-, 0.43,-,-,-, 0.35,-, 0.41,-, 0.24,-,-,-,-, 0.26,-,-,-,-, 0.32,-, 0.37,-,-,-, 0.33,-,-,-, 0.39,-,-,-,-,-, 0.24,-, 0.34, 0.31, 0.28,-, 0.41, 0.21,-,-, 0.32, 0.38, 0.36,-,-,-,-,-, 0.35,-,-,-,-,-, 0.42,-, 0.19,-,-,-,-, 0.32, 0.27, 0.40,-,-, 0.07, 0.07,-, lr of zeroth group 2.0791666666666666e-05 data time  5.30 step time  0.91 forward time  0.30 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.18395721912384033, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.17364957928657532, nan, nan, nan, nan, 0.3754514157772064, nan, nan, nan, nan, nan, 0.3081427216529846, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.14117716252803802, nan, nan, nan, 0.19066275656223297, nan, nan, nan, nan, nan, nan, nan, 0.23436513543128967, nan, 0.2655273675918579, nan, nan, 0.159658744931221, nan, nan, 0.1571243852376938, nan, nan, nan, nan, nan, 0.13647377490997314, nan, 0.18080352246761322, nan, nan, nan, 0.2106335163116455, nan, nan, nan, nan, nan, nan, nan, 0.18095064163208008, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.2613134980201721, nan, nan, nan, nan, 0.20618650317192078, nan, nan, nan, nan, nan, nan, 0.40907105803489685, 0.189009889960289, 0.27214372158050537, nan, nan, nan, nan, nan, nan, nan, 0.2915583550930023, 0.26222461462020874, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.15165643393993378, nan, nan, nan, 0.35702264308929443, nan, nan, nan, nan, nan, nan, 0.2701466977596283, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.22835606336593628, nan, 0.2683652341365814, 0.2455768585205078, 0.14629583060741425, nan, nan, nan, nan, 0.30257493257522583, nan, nan, 4.340084075927734, nan, nan, 0.3529834747314453, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.2583266794681549, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.2446821630001068, nan, nan, nan, nan, nan, nan, 0.2585819661617279, nan, nan, nan, nan, 0.19287250936031342, nan, nan, nan, nan, nan, nan, 0.2995322346687317, nan, nan, nan, nan, nan, nan, nan, 0.17054066061973572, nan, 0.1442360281944275, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.2451721727848053, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.23626038432121277, nan, 0.16189248859882355, nan, nan, 0.17303134500980377, nan, nan, nan, 0.33360758423805237, nan, nan, nan, nan, nan, nan, 0.21248266100883484, nan, nan, nan, nan, 0.16462063789367676, nan, nan, nan, nan, 0.29226845502853394, nan, 0.24447448551654816, nan, nan, nan, nan, nan, nan, 0.28222060203552246, nan, nan, nan, nan, 0.1602579951286316, 0.29385754466056824, nan, 0.19728374481201172, 0.3231284022331238, nan, 0.30037569999694824, nan, nan, nan, nan, nan, 0.2639603614807129, 0.22644773125648499, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.4676204323768616, nan, 0.4524844288825989, nan, nan, nan, 0.16234040260314941, 0.17943015694618225, nan, 0.20025475323200226, nan, nan, 0.1923094540834427, nan, 0.22149306535720825, nan, nan, nan, nan, nan, nan, 0.6650911569595337, 0.1065521240234375, nan, nan, nan, 0.2717863917350769, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.18607164919376373, nan, nan, nan, nan, nan, 0.188670814037323, nan, 0.18832272291183472, nan, nan, nan, 0.21361252665519714, nan, nan, nan, nan, nan, nan, 0.15657082200050354, nan, nan, 0.1914171576499939, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.21265660226345062, nan, nan, nan, nan, nan, nan, nan, nan, 0.272780179977417, nan, 0.18494437634944916, 0.2593362033367157, 0.1490471512079239, 0.10753682255744934, nan, 0.27192211151123047, nan, nan, 0.711595356464386, 0.16288788616657257, 0.1307060271501541, nan, 0.41135698556900024, 0.20604081451892853, 0.2539432942867279, 0.3789733946323395, 0.16651910543441772, 0.21034474670886993, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.23589995503425598, 0.4288891553878784, nan]
| end of epoch   4 | time: 639.14s | mean loss  0.28 | pos losses -,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.17,-,-,-,-, 0.38,-,-,-,-,-, 0.31,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.14,-,-,-, 0.19,-,-,-,-,-,-,-, 0.23,-, 0.27,-,-, 0.16,-,-, 0.16,-,-,-,-,-, 0.14,-, 0.18,-,-,-, 0.21,-,-,-,-,-,-,-, 0.18,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-, 0.21,-,-,-,-,-,-, 0.41, 0.19, 0.27,-,-,-,-,-,-,-, 0.29, 0.26,-,-,-,-,-,-,-,-,-,-, 0.15,-,-,-, 0.36,-,-,-,-,-,-, 0.27,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.23,-, 0.27, 0.25, 0.15,-,-,-,-, 0.30,-,-, 4.34,-,-, 0.35,-,-,-,-,-,-,-,-,-,-,-,-,-,-, 0.26,-,-,-,-,-,-,-,-,-, 0.24,-,-,-,-,-,-, 0.26,-,-,-,-, 0.19,-,-,-,-,-,-, 0.30,-,-,-,-,-,-,-, 0.17,-, 0.14,-,-,-,-,-,-,-,-,-, 0.25,-,-,-,-,-,-,-,-,-, 0.24,-, 0.16,-,-, 0.17,-,-,-, 0.33,-,-,-,-,-,-, 0.21,-,-,-,-, 0.16,-,-,-,-, 0.29,-, 0.24,-,-,-,-,-,-, 0.28,-,-,-,-, 0.16, 0.29,-, 0.20, 0.32,-, 0.30,-,-,-,-,-, 0.26, 0.23,-,-,-,-,-,-,-,-,-, 0.47,-, 0.45,-,-,-, 0.16, 0.18,-, 0.20,-,-, 0.19,-, 0.22,-,-,-,-,-,-, 0.67, 0.11,-,-,-, 0.27,-,-,-,-,-,-,-,-,-, 0.19,-,-,-,-,-, 0.19,-, 0.19,-,-,-, 0.21,-,-,-,-,-,-, 0.16,-,-, 0.19,-,-,-,-,-,-,-,-,-, 0.21,-,-,-,-,-,-,-,-, 0.27,-, 0.18, 0.26, 0.15, 0.11,-, 0.27,-,-, 0.71, 0.16, 0.13,-, 0.41, 0.21, 0.25, 0.38, 0.17, 0.21,-,-,-,-,-,-,-,-,-,-,-, 0.24, 0.43,-, lr of zeroth group 2.4958333333333338e-05 data time  5.34 step time  1.06 forward time  0.34 nan share  0.00 ignore share (for classification tasks) 0.0000
-----------------------------------------------------------------------------------------
